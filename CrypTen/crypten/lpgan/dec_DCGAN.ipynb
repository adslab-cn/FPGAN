{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Size of Dataset: 50000\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import utils, datasets, transforms, models\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import pickle\n",
    "import crypten\n",
    "import crypten.nn as nn\n",
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "dataroot = \"../data/MNIST\"  # 数据集所在的路径，我们已经事先下载下来了\n",
    "workers = 10  # 数据加载时的进程数\n",
    "batch_size = 64  # 生成器输入的大小\n",
    "\n",
    "image_size = 64  # 训练图像的大小\n",
    "nc = 1  # 训练图像的通道数，彩色图像的话就是 3\n",
    "nz = 100  # 输入是100 维的随机噪声 z，看作是 100 个 channel，每个特征图宽高是 1*1\n",
    "ngf = 64  # 生成器中特征图的大小，\n",
    "ndf = 64  # 判别器中特征图的大小\n",
    "num_epochs = 1  # 训练的轮次\n",
    "# num_epochs = 10  # 训练的轮次\n",
    "lr = 0.0005  # 学习率大小\n",
    "beta1 = 0.5  # Adam 优化器的参数\n",
    "ngpu = 1  # 可用 GPU  的个数，0 代表使用 CPU\n",
    "\n",
    "# 训练集加载并进行归一化等操作\n",
    "train_data = datasets.MNIST(\n",
    "    root=dataroot,\n",
    "    train=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, ), (0.5, ))\n",
    "    ]),\n",
    "    download=True  # 从互联网下载，如果已经下载的话，就直接使用\n",
    ")\n",
    "\n",
    "# 测试集加载并进行归一化等操作\n",
    "test_data = datasets.MNIST(root=dataroot,\n",
    "                        train=False,\n",
    "                        transform=transforms.Compose([\n",
    "                            transforms.Resize(image_size),\n",
    "                            transforms.ToTensor(),\n",
    "                            transforms.Normalize((0.5, ), (0.5, ))\n",
    "                        ]))\n",
    "\n",
    "# 把 MNIST 的训练集和测试集都用来做训练\n",
    "# dataset = train_data + test_data\n",
    "# dataset = train_data\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "# 选择训练集的前 5000 张数据\n",
    "subset_indices = list(range(50000))  # 创建索引范围\n",
    "dataset = Subset(train_data, subset_indices)\n",
    "\n",
    "print(f'Total Size of Dataset: {len(dataset)}')\n",
    "\n",
    "# 数据加载器，训练过程中不断产生数据\n",
    "dataloader = DataLoader(dataset=dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=workers)\n",
    "\n",
    "# 看是否存在可用的 GPU\n",
    "device = torch.device('cuda:0' if (\n",
    "    torch.cuda.is_available() and ngpu > 0) else 'cpu')\n",
    "\n",
    "# 权重初始化函数\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "class Generator(crypten.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.decon1 = crypten.nn.ConvTranspose2d(in_channels=nz, \n",
    "                                                 out_channels=ngf*8,\n",
    "                                                 kernel_size=4, \n",
    "                                                 stride=1, \n",
    "                                                 padding=0,\n",
    "                                                 bias=False)\n",
    "        self.bn1 = crypten.nn.BatchNorm2d(ngf*8)\n",
    "        self.decon2 = crypten.nn.ConvTranspose2d(ngf*8, ngf*4, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn2 = crypten.nn.BatchNorm2d(ngf*4)\n",
    "        self.decon3 = crypten.nn.ConvTranspose2d(ngf*4, ngf*2, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn3 = crypten.nn.BatchNorm2d(ngf*2)\n",
    "        self.decon4 = crypten.nn.ConvTranspose2d(ngf*2, ngf, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn4 = crypten.nn.BatchNorm2d(ngf)\n",
    "        self.decon5 = crypten.nn.ConvTranspose2d(ngf, nc, 4, stride=2, padding=1, bias=False)\n",
    "        self.relu = crypten.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.decon1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(self.decon2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(self.decon3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(self.decon4(x))\n",
    "        x = self.bn4(x)\n",
    "        x = self.decon5(x)\n",
    "        x = x.tanh()\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class Discriminator(crypten.nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nc = nc \n",
    "        self.ndf = ndf\n",
    "        self.conv1 = crypten.nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False)  # (64, 14, 14)\n",
    "        self.leaky_relu = crypten.nn.LeakyRelu(0.2)\n",
    "        self.conv2 = crypten.nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False)  # (128, 7, 7)\n",
    "        self.bn1 = crypten.nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = crypten.nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False)  # (256, 3, 3)\n",
    "        self.bn2 = crypten.nn.BatchNorm2d(ndf * 4)\n",
    "        # self.conv4 = crypten.nn.Conv2d(ndf * 4, 1, 4, stride=1, padding=0, bias=False)  # (1, 1, 1)\n",
    "        self.conv4 = crypten.nn.Conv2d(ndf * 4, 1, 1, stride=1, padding=0, bias=False)  # (1, 1, 1)\n",
    "        self.sd = crypten.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.leaky_relu(self.bn2(self.conv3(x)))\n",
    "        x = self.sd(self.conv4(x))\n",
    "\n",
    "        return x.view(x.size(0), -1).mean(dim=1)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate losses\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "        \n",
    "    # numerically stable loss\n",
    "    criterion = crypten.nn.BCELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    # labels = torch.zeros(batch_size)*0.9 # new\n",
    "    criterion = crypten.nn.BCELoss()\n",
    "    # criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# 加载预训练的Inception模型\n",
    "inception_model = models.inception_v3(weights='DEFAULT', transform_input=False)  # 使用 weights 代替 pretrained\n",
    "inception_model.eval()\n",
    "\n",
    "# 定义图像预处理操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 定义Inception Score计算函数\n",
    "def inception_score(img_list, inception_model, num_splits=10):\n",
    "    inception_model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img in img_list:\n",
    "            # 如果图像是单通道，扩展为三个通道\n",
    "            if img.size(0) == 1:  # 检查是否为单通道图像\n",
    "                img = img.expand(3, -1, -1)  # 复制单通道数据到三个通道\n",
    "            \n",
    "            img = transform(img)\n",
    "            img = img.unsqueeze(0)  # 增加batch维度\n",
    "            img = img.to('cpu')  # 如果inception模型在cpu上\n",
    "            pred = inception_model(img)[0]\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = torch.stack(preds)\n",
    "    preds = F.softmax(preds, dim=1)\n",
    "    split_scores = []\n",
    "    for k in range(num_splits):\n",
    "        part = preds[k * (len(preds) // num_splits): (k + 1) * (len(preds) // num_splits), :]\n",
    "        p_y = part.mean(0)\n",
    "        kl_divergence = part * (torch.log(part) - torch.log(p_y.unsqueeze(0)))\n",
    "        kl_divergence = kl_divergence.sum(1)\n",
    "        split_scores.append(kl_divergence.mean().exp())\n",
    "\n",
    "    return torch.tensor(split_scores).mean().item(), torch.tensor(split_scores).std().item()\n",
    "\n",
    "def construct_private_model(modelD, modelG):\n",
    "    \"\"\"Encrypt and validate trained model for multi-party setting.\"\"\"\n",
    "    # get rank of current process\n",
    "    rank = comm.get().get_rank()\n",
    "    dummy_input = torch.empty(1, 1, 28, 28)\n",
    "    \n",
    "    # party 0 always gets the actual model; remaining parties get dummy model\n",
    "    if rank == 0:\n",
    "        D = modelD\n",
    "        # G = modelG\n",
    "        G = Generator(ngpu=0)\n",
    "        # G = Generator(ngpu).to(device)\n",
    "        # add_state_dict_hooks(G)\n",
    "    else:\n",
    "        # 占位用的，就和份额的broadcast一个道理\n",
    "        # need to provide a dummy model to tell CrypTen the model's structure\n",
    "        D = Discriminator(ngpu=0)\n",
    "        # D = Discriminator(ngpu).to(device)\n",
    "        # add_state_dict_hooks(D)\n",
    "        # G = Generator()\n",
    "        G = modelG\n",
    "        \n",
    "    private_modelD = D.encrypt(src=0)\n",
    "    private_modelG = G.encrypt(src=0)\n",
    "    # private_modelG = G.encrypt(src=1) # 如果一方是G，一方是D会不会内存会小一点？\n",
    "    \n",
    "    # private_modelD = crypten.nn.from_pytorch(D, dummy_input)\n",
    "    # private_modelG = crypten.nn.from_pytorch(G, dummy_input)\n",
    "    # print(\"pr_D\", private_modelD)\n",
    "    return private_modelD.encrypt(), private_modelG.encrypt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def run():\n",
    "    # 定义一些变量，用来存储每轮的相关值\n",
    "    img_list = []\n",
    "    img = []\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    D_x_list = []\n",
    "    D_z_list = []\n",
    "    loss_tep = 10\n",
    "\n",
    "    \n",
    "    # 创建一个生成器对象\n",
    "    netG = Generator(ngpu=0)\n",
    "\n",
    "    # 初始化权重  其中，mean=0, stdev=0.2.\n",
    "    # netG.apply(weights_init)\n",
    "    for layer in netG.children():\n",
    "        weights_init(layer)\n",
    "    \n",
    "    \n",
    "    # 创建一个判别器对象\n",
    "    netD = Discriminator(ngpu=0)\n",
    "\n",
    "    # 初始化权重  其中，mean=0, stdev=0.2.\n",
    "    # netD.apply(weights_init)\n",
    "    for layer in netD.children():\n",
    "        weights_init(layer)\n",
    "\n",
    "    netD, netG = construct_private_model(netD, netG) \n",
    "    print(\"==================\")\n",
    "\n",
    "    # 创建一批潜在向量，我们将使用它们来可视化生成器的生成过程\n",
    "    fixed_noise = torch.randn(100, nz, 1, 1)\n",
    "    fixed_noise = crypten.cryptensor(fixed_noise)\n",
    "\n",
    "    real_label = 1.  # “真”标签\n",
    "    fake_label = 0.  # “假”标签\n",
    "\n",
    "    optimizerG = crypten.optim.SGD(netG.parameters(), lr=1e-2)\n",
    "    optimizerD = crypten.optim.SGD(netD.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "    print(\"Starting Training Loop...\")\n",
    "    # 迭代\n",
    "    for epoch in range(num_epochs):\n",
    "        beg_time = time.time()\n",
    "        epoch_error = 0  # 初始化epoch误差\n",
    "        # 数据加载器读取数据\n",
    "        for i, data in enumerate(dataloader):\n",
    "            ############################\n",
    "            # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "            ###########################\n",
    "            # 用所有的真数据进行训练\n",
    "            netD.zero_grad()\n",
    "            # Format batch\n",
    "            real_cpu = data[0]\n",
    "            # print(\"data's size:\", real_cpu.size())\n",
    "            b_size = real_cpu.size(0)\n",
    "            label = torch.full((b_size, ),\n",
    "                            real_label,\n",
    "                            dtype=torch.float,\n",
    "                            )\n",
    "\n",
    "            # 判别器推理\n",
    "            real_cpu = crypten.cryptensor(real_cpu)\n",
    "            output = netD(real_cpu).view(-1)\n",
    "            # Calculate loss on all-real batch\n",
    "            # 计算所有真标签的损失函数\n",
    "            # errD_real = criterion(output, label)\n",
    "            errD_real = real_loss(output, smooth=True)\n",
    "\n",
    "            # Calculate gradients for D in backward pass\n",
    "            errD_real.backward()\n",
    "\n",
    "            D_x = output.mean().item()\n",
    "\n",
    "            # 生成假数据并进行训练\n",
    "            noise = torch.randn(b_size, nz, 1, 1)\n",
    "            noise = crypten.cryptensor(noise)\n",
    "\n",
    "            # 用生成器生成假图像\n",
    "            g_time = time.time()\n",
    "            fakeG = netG(noise)\n",
    "            g_end_time = time.time()\n",
    "            # print(\"SG-FP:\",fake)\n",
    "            # print(\"Runtime\\nSynthesis:\", g_end_time-g_time)\n",
    "            label.fill_(fake_label)\n",
    "            # Classify all fake batch with D\n",
    "            d_time = time.time()\n",
    "            output = netD(fakeG.detach()).view(-1)\n",
    "            # print(\"SD-FP:\",output)\n",
    "            d_end_time = time.time()\n",
    "            # print(\"Runtime\\nInference:\", d_end_time-d_time)\n",
    "\n",
    "            # 计算判别器在假数据上的损失\n",
    "            # errD_fake = criterion(output, label)\n",
    "            errD_fake = fake_loss(output)\n",
    "            \n",
    "            errD_fake.backward()\n",
    "\n",
    "            D_G_z1 = output.mean().item()\n",
    "            # Add the gradients from the all-real and all-fake batches\n",
    "            errD = errD_real + errD_fake \n",
    "            # errD = errD_real + errD_fake * 0.5 # 削弱对假样本的惩罚\n",
    "            # errD.backward()\n",
    "            \n",
    "\n",
    "            # Update D\n",
    "            optimizerD.step()\n",
    "\n",
    "            # print(f\"Discriminator Gradient Norm: {d_grad_norm:.4f}\")\n",
    "            # if step % 2 == 0:\n",
    "            #     optimizerD.step()\n",
    "            # optimizerG.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: maximize log(D(G(z)))\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)  # fake labels are real for generator cost\n",
    "            # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "            output = netD(fakeG).view(-1)\n",
    "            # Calculate G's loss based on this output\n",
    "            # errG = criterion(output, label)\n",
    "            \n",
    "            errG = real_loss(output)\n",
    "            # errG = real_loss(output) * 0.5 # 减弱生成器压力\n",
    "            \n",
    "            errG.backward()\n",
    "            # # ====================\n",
    "            # # 第二次训练Generator\n",
    "            # # ====================\n",
    "            # # 再次更新生成器\n",
    "            # netG.zero_grad()\n",
    "            # output = netD(fakeG).view(-1)  # 使用更新后的判别器\n",
    "            # errG = real_loss(output)\n",
    "            # errG.backward()  # 生成器的第二次反向传播\n",
    "            # optimizerG.step()\n",
    "            # # =========================\n",
    "\n",
    "\n",
    "            D_G_z2 = output.mean().item()\n",
    "            # Update G\n",
    "            optimizerG.step()\n",
    "\n",
    "            # print(f\"Discriminator Gradient Norm: {d_grad_norm:.4f}\")\n",
    "\n",
    "            # Output training stats\n",
    "            end_time = time.time()\n",
    "            run_time = round(end_time - beg_time)\n",
    "            print(f'Epoch: [{epoch+1:0>{len(str(num_epochs))}}/{num_epochs}]',\n",
    "                f'Step: [{i+1:0>{len(str(len(dataloader)))}}/{len(dataloader)}]',\n",
    "                f'Loss-D: {errD.get_plain_text().item():.4f}',\n",
    "                f'Loss-G: {errG.get_plain_text().item():.4f}',\n",
    "                f'D(x): {D_x:.4f}',\n",
    "                f'D(G(z)): [{D_G_z1:.4f}/{D_G_z2:.4f}]',\n",
    "                f'Time: {run_time}s',\n",
    "                end='\\r')\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(errG.get_plain_text().item())\n",
    "            D_losses.append(errD.get_plain_text().item())\n",
    "\n",
    "            \n",
    "            # Save D(X) and D(G(z)) for plotting later\n",
    "            D_x_list.append(D_x.get_plain_text())\n",
    "            D_z_list.append(D_G_z2.get_plain_text())\n",
    "\n",
    "            # 保存最好的模型\n",
    "            if errG.get_plain_text() < loss_tep:\n",
    "                torch.save(netG.state_dict(), 'model.pt')\n",
    "                temp = errG\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise).detach().cpu()\n",
    "            fake = fake.get_plain_text()\n",
    "        img_list.append(utils.make_grid(fake * 0.5 + 0.5, nrow=10))\n",
    "        img.extend(fake)  # 保存每张生成图像\n",
    "        print()\n",
    "\n",
    "        # Size of the Figure\n",
    "        plt.figure(figsize=(20, 10))\n",
    "\n",
    "        # Plot the real images\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Real Images\")\n",
    "        real = next(iter(dataloader))\n",
    "        plt.imshow(\n",
    "            utils.make_grid(real[0][:100] * 0.5 + 0.5, nrow=10).permute(1, 2, 0))\n",
    "\n",
    "        # Load the Best Generative Model\n",
    "        netG = Generator(0)\n",
    "        netG.load_state_dict(torch.load('model.pt', map_location=torch.device('cpu')))\n",
    "        netG.eval()\n",
    "\n",
    "        # Generate the Fake Images\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise.cpu())\n",
    "            fake = fake.get_plain_text()\n",
    "\n",
    "        # Plot the fake images\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(\"Fake Images\")\n",
    "        fake = utils.make_grid(fake * 0.5 + 0.5, nrow=10)\n",
    "        plt.imshow(fake.permute(1, 2, 0))\n",
    "\n",
    "        # Save the comparation result\n",
    "        plt.savefig('result/result_dec.jpg', bbox_inches='tight')\n",
    "\n",
    "        # 在训练循环结束后评估Inception Score\n",
    "        mean_inception_score, std_inception_score = inception_score(img, inception_model)\n",
    "        print(f\"Inception Score: {mean_inception_score:.4f} ± {std_inception_score:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Project/PR-crypten/CrypTen/crypten/mpc/context.py:97\u001b[0m, in \u001b[0;36mrun_multiprocess.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m     process\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m process \u001b[38;5;129;01min\u001b[39;00m processes:\n\u001b[0;32m---> 97\u001b[0m     \u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m was_initialized:\n\u001b[1;32m    100\u001b[0m     crypten\u001b[38;5;241m.\u001b[39minit()\n",
      "File \u001b[0;32m~/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py:149\u001b[0m, in \u001b[0;36mBaseProcess.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_pid \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetpid(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a child process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcan only join a started process\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 149\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     _children\u001b[38;5;241m.\u001b[39mdiscard(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/sl/lib/python3.8/multiprocessing/popen_fork.py:47\u001b[0m, in \u001b[0;36mPopen.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# This shouldn't block if wait() returned successfully.\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWNOHANG\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode\n",
      "File \u001b[0;32m~/anaconda3/envs/sl/lib/python3.8/multiprocessing/popen_fork.py:27\u001b[0m, in \u001b[0;36mPopen.poll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 27\u001b[0m         pid, sts \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;66;03m# Child process not yet created. See #1731717\u001b[39;00m\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;66;03m# e.errno == errno.ECHILD == 10\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
