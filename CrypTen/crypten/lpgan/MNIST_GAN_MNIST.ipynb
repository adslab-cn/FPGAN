{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "from torchvision import utils, datasets, transforms, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import torch\n",
    "import torchvision\n",
    "from crypten import mpc\n",
    "\n",
    "crypten.init()\n",
    "torch.set_num_threads(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最大梯度差异: 3.5070250305579975e-05\n",
      "初始权重差异: 0.06216694414615631\n",
      "参数差异: 0.0\n",
      "参数差异: 0.0\n",
      "数据加密差异: 1.5228986740112305e-05\n",
      "PyTorch Loss: 0.6937922239303589\n",
      "Crypten Loss: 0.7220306396484375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import crypten\n",
    "\n",
    "\n",
    "# 对比测试\n",
    "def test_gradient_consistency():\n",
    "    # 输入设置\n",
    "    x = torch.randn(2, 3, 16, 16, requires_grad=True)\n",
    "    target = torch.randn(2, 64, 32, 32)\n",
    "    \n",
    "    # 原生实现\n",
    "    conv_native = nn.ConvTranspose2d(3, 64, 4, 2, 1)\n",
    "    loss_native = ((conv_native(x) - target)**2).mean()\n",
    "    loss_native.backward()\n",
    "    grad_native = x.grad.clone()\n",
    "    \n",
    "    # 自定义实现\n",
    "    x.grad = None  # 清空梯度\n",
    "    conv_custom = crypten.nn.ConvTranspose2d(3, 64, 4, 2, 1)\n",
    "    conv_custom.encrypt()\n",
    "    \n",
    "    # 将输入和目标转换为加密张量\n",
    "    x_enc = crypten.cryptensor(x, requires_grad=True)\n",
    "    target_enc = crypten.cryptensor(target)\n",
    "    \n",
    "    # 前向传播\n",
    "    output_custom = conv_custom(x_enc)\n",
    "    loss_custom = (output_custom - target_enc).pow(2).mean()\n",
    "    \n",
    "    # 反向传播\n",
    "    loss_custom.backward()\n",
    "    \n",
    "    # 获取加密输入的梯度\n",
    "    grad_custom_enc = x_enc.grad\n",
    "    grad_custom = grad_custom_enc.get_plain_text()  # 解密梯度\n",
    "    \n",
    "    # 误差分析\n",
    "    diff = (grad_native - grad_custom).abs().max()\n",
    "    print(f\"最大梯度差异: {diff.item()}\")\n",
    "    \n",
    "    # PyTorch 模型权重\n",
    "    weight_pytorch = conv_native.weight.detach()\n",
    "\n",
    "    # Crypten 模型权重\n",
    "    weight_crypten = conv_custom.weight.get_plain_text()\n",
    "\n",
    "    # 对比差异\n",
    "    diff_weight = (weight_pytorch - weight_crypten).abs().max()\n",
    "    print(f\"初始权重差异: {diff_weight.item()}\")\n",
    "\n",
    "    # PyTorch 优化器\n",
    "    optimizer_pytorch = torch.optim.SGD(conv_native.parameters(), lr=0.0002)\n",
    "\n",
    "    # Crypten 优化器\n",
    "    optimizer_crypten = crypten.optim.SGD(conv_native.parameters(), lr=0.0002)\n",
    "\n",
    "    # 对比参数更新\n",
    "    for param_pytorch, param_crypten in zip(conv_native.parameters(), conv_native.parameters()):\n",
    "        diff_param = (param_pytorch - param_crypten).abs().max()\n",
    "        print(f\"参数差异: {diff_param.item()}\")\n",
    "\n",
    "    x_enc = crypten.cryptensor(x)\n",
    "    diff_data = (x - x_enc.get_plain_text()).abs().max()\n",
    "    print(f\"数据加密差异: {diff_data.item()}\")\n",
    "\n",
    "    criterionp = torch.nn.BCELoss()\n",
    "    criterionc = crypten.nn.BCELoss()\n",
    "    loss_pytorch = criterionp(conv_native(x).sigmoid(), target)\n",
    "    loss_crypten = criterionc(output_custom.sigmoid(), crypten.cryptensor(target))\n",
    "    print(f\"PyTorch Loss: {loss_pytorch.item()}\")\n",
    "    print(f\"Crypten Loss: {loss_crypten.get_plain_text().item()}\")\n",
    "\n",
    "test_gradient_consistency()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the training dataset: 60000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "nc = 1  # 训练图像的通道数，彩色图像的话就是 3\n",
    "nz = 100  # 输入是100 维的随机噪声 z，看作是 100 个 channel，每个特征图宽高是 1*1\n",
    "ngf = 64  # 生成器中特征图的大小，\n",
    "ndf = 64  # 判别器中特征图的大小\n",
    "workers = 10  # 数据加载时的进程数\n",
    "batch_size = 64  # 生成器输入的大小\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root='../data', \n",
    "                                           train=True, \n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "num_samples = len(train_data)\n",
    "print(f\"Number of samples in the training dataset: {num_samples}\")\n",
    "\n",
    "# 数据加载器，训练过程中不断产生数据\n",
    "dataloader = DataLoader(dataset=train_data,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in the test dataset: 10000\n"
     ]
    }
   ],
   "source": [
    "test_data = torchvision.datasets.MNIST(root='../data', \n",
    "                                           train=False, \n",
    "                                           transform=torchvision.transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "num_samples = len(test_data)\n",
    "print(f\"Number of samples in the test dataset: {num_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### celeba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataroot = \"../data/celeba/Img\"  # 数据集所在的路径\n",
    "# workers = 10  # 数据加载时的进程数\n",
    "# batch_size = 64  # 生成器输入的大小\n",
    "\n",
    "# image_size = 64  # 训练图像的大小\n",
    "# nc = 3  # 训练图像的通道数，CelebA是彩色图像\n",
    "# nz = 100  # 输入是100维的随机噪声 z\n",
    "# ngf = 64  # 生成器中特征图的大小\n",
    "# ndf = 64  # 判别器中特征图的大小\n",
    "\n",
    "# train_data = torchvision.datasets.ImageFolder(root=dataroot,\n",
    "#                            transform=transforms.Compose([\n",
    "#                                transforms.Resize(image_size),\n",
    "#                                transforms.CenterCrop(image_size),\n",
    "#                                transforms.ToTensor(),\n",
    "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#                            ]))\n",
    "# # Create the dataloader\n",
    "# dataloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "#                                          shuffle=True, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataroot = \"../data/CIFAR10\"  # 数据集所在的路径\n",
    "# workers = 10  # 数据加载时的进程数\n",
    "# batch_size = 64  # 生成器输入的大小\n",
    "\n",
    "# image_size = 64  # 训练图像的大小\n",
    "# nc = 3  # 训练图像的通道数，CelebA是彩色图像\n",
    "# nz = 100  # 输入是100维的随机噪声 z\n",
    "# ngf = 64  # 生成器中特征图的大小\n",
    "# ndf = 64  # 判别器中特征图的大小\n",
    "# num_epochs = 5  # 训练的轮次\n",
    "# lr = 0.0005  # 学习率大小\n",
    "# beta1 = 0.5  # Adam 优化器的参数\n",
    "# ngpu = 1  # 可用 GPU 的个数，0 代表使用 CPU\n",
    "\n",
    "# # 训练集加载并进行归一化等操作\n",
    "# train_data = datasets.CIFAR10(root=dataroot, download=True,\n",
    "#                            transform=transforms.Compose([\n",
    "#                                transforms.Resize(image_size),\n",
    "#                                transforms.ToTensor(),\n",
    "#                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#                            ]))\n",
    "\n",
    "# # 数据加载器，训练过程中不断产生数据\n",
    "# dataloader = torch.utils.data.DataLoader(dataset=train_data,\n",
    "#                         batch_size=batch_size,\n",
    "#                         shuffle=True,\n",
    "#                         num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = next(iter(dataloader))[0]\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# plt.title(\"Training Images\")\n",
    "# plt.axis('off')\n",
    "# inputs = utils.make_grid(inputs[:100] * 0.5 + 0.5, nrow=10)\n",
    "# plt.imshow(inputs.permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_samples(digits, n_samples=60000):\n",
    "    \"\"\"Returns images and labels based on sample size\"\"\"\n",
    "    images, labels = [], []\n",
    "\n",
    "    for i, digit in enumerate(digits):\n",
    "        if i == n_samples:\n",
    "            break\n",
    "        image, label = digit\n",
    "        images.append(image)\n",
    "        label_one_hot = torch.nn.functional.one_hot(torch.tensor(label), 10)\n",
    "        labels.append(label_one_hot)\n",
    "\n",
    "    images = torch.cat(images)\n",
    "    labels = torch.stack(labels)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = take_samples(train_data, n_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 28, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = images.size(0)\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Alice and Bob's Train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alice is party 0\n",
    "ALICE = 0\n",
    "BOB = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_alice.size():sample_alice.size():  torch.Size([500, 28, 28])torch.Size([500, 28, 28])\n",
      "\n",
      "sample_bob.size(): sample_bob.size():torch.Size([500, 28, 28]) \n",
      "torch.Size([500, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def save_digits():\n",
    "    sample_alice = images[ :500]\n",
    "    sample_bob = images[500:1000]\n",
    "    print(\"sample_alice.size():\", sample_alice.size())\n",
    "    print(\"sample_bob.size():\", sample_bob.size())\n",
    "\n",
    "    crypten.save_from_party(sample_alice, \"../data/alice_mnist.pth\", src=ALICE)\n",
    "    crypten.save_from_party(sample_bob, \"../data/bob_mnist.pth\", src=BOB)\n",
    "      \n",
    "save_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/tmp/data': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "! ls -lh /tmp/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator hyperparams\n",
    "\n",
    "# Size of input image to discriminator (28*28)\n",
    "input_size = 784\n",
    "# Size of discriminator output (real or fake)\n",
    "d_output_size = 1\n",
    "# Size of last hidden layer in the discriminator\n",
    "d_hidden_size = 32\n",
    "\n",
    "# Generator hyperparams\n",
    "\n",
    "# Size of latent vector to give to generator\n",
    "nz = 100\n",
    "# Size of discriminator output (generated image)\n",
    "g_output_size = 784\n",
    "# Size of first hidden layer in the generator\n",
    "g_hidden_size = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator and Discriminator losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate losses\n",
    "import crypten.nn\n",
    "\n",
    "def real_loss(D_out, smooth=False):\n",
    "    batch_size = D_out.size(0)\n",
    "    # label smoothing\n",
    "    if smooth:\n",
    "        # smooth, real labels = 0.9\n",
    "        labels = torch.ones(batch_size)*0.9\n",
    "    else:\n",
    "        labels = torch.ones(batch_size) # real labels = 1\n",
    "        \n",
    "    # numerically stable loss\n",
    "    # criterion = crypten.nn.BCELoss()\n",
    "    criterion = crypten.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss\n",
    "\n",
    "def fake_loss(D_out):\n",
    "    batch_size = D_out.size(0)\n",
    "    labels = torch.zeros(batch_size) # fake labels = 0\n",
    "    # criterion = crypten.nn.BCELoss()\n",
    "    criterion = crypten.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # calculate loss\n",
    "    loss = criterion(D_out.squeeze(), labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结构+加密一次性解决"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import crypten.nn as nn\n",
    "import torch\n",
    "\n",
    "class Generator(crypten.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.decon1 = crypten.nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0,bias=False)\n",
    "        self.bn1 = crypten.nn.BatchNorm2d(512)\n",
    "        self.decon2 = crypten.nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn2 = crypten.nn.BatchNorm2d(256)\n",
    "        self.decon3 = crypten.nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn3 = crypten.nn.BatchNorm2d(128)\n",
    "        self.decon4 = crypten.nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn4 = crypten.nn.BatchNorm2d(64)\n",
    "        self.decon5 = crypten.nn.ConvTranspose2d(64, 1, 4, stride=2, padding=1, bias=False)\n",
    "        self.relu = crypten.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.decon1(x))\n",
    "        x = self.bn1(x)\n",
    "        print(f\"Shape after decon1: {x.shape}\")\n",
    "        x = self.relu(self.decon2(x))\n",
    "        x = self.bn2(x)\n",
    "        print(f\"Shape after deconv2: {x.shape}\")\n",
    "        x = self.relu(self.decon3(x))\n",
    "        x = self.bn3(x)\n",
    "        print(f\"Shape after deconv3: {x.shape}\")\n",
    "        x = self.relu(self.decon4(x))\n",
    "        print(f\"Shape after deconv4: {x.shape}\")\n",
    "        x = self.bn4(x)\n",
    "        x = self.decon5(x)\n",
    "        print(f\"Shape after deconv5: {x.shape}\")\n",
    "        x = x.tanh()\n",
    "        return x\n",
    "\n",
    "class Discriminator(crypten.nn.Module):\n",
    "\n",
    "    def __init__(self, nc, ndf):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.nc = nc \n",
    "        self.ndf = ndf\n",
    "\n",
    "        # self.nc(1 or 3) * 64 * 64\n",
    "        self.conv1 = crypten.nn.Conv2d(self.nc, self.ndf, 4, stride=2, padding=1, bias=False)\n",
    "        self.leaky_relu = crypten.nn.LeakyReLU(0.2)\n",
    "        # self.ndf(64) * 32 * 32\n",
    "        self.conv2 = crypten.nn.Conv2d(self.ndf, self.ndf * 2, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = crypten.nn.BatchNorm2d(self.ndf * 2)\n",
    "\n",
    "        # self.ndf*2(128) * 16 * 16\n",
    "        self.conv3 = crypten.nn.Conv2d(self.ndf * 2, self.ndf * 4, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = crypten.nn.BatchNorm2d(self.ndf * 4)\n",
    "        # self.ndf*4(256) * 8 * 8\n",
    "        self.conv4 = crypten.nn.Conv2d(self.ndf * 4, self.ndf * 8, 4, stride=2, padding=1, bias=False)\n",
    "        self.bn3 = crypten.nn.BatchNorm2d(self.ndf * 8)\n",
    "        # self.ndf*8(512) * 4 * 4, stride = 1\n",
    "        # self.conv5 = crypten.nn.Conv2d(self.ndf * 8, 1, 4, stride=1, padding=0, bias=False) # 如果输入是64x64\n",
    "        self.conv5 = crypten.nn.Conv2d(self.ndf * 8, 1, 1, stride=1, padding=0, bias=False) # 如果输入是28x28\n",
    "        self.sd = crypten.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        print(f\"Shape after conv1: {x.shape}\")\n",
    "        x = self.leaky_relu(self.bn1(self.conv2(x)))\n",
    "        print(f\"Shape after conv2: {x.shape}\")\n",
    "        x = self.leaky_relu(self.bn2(self.conv3(x)))\n",
    "        print(f\"Shape after conv3: {x.shape}\")\n",
    "        x = self.leaky_relu(self.bn3(self.conv4(x)))\n",
    "        print(f\"Shape after conv4: {x.shape}\")\n",
    "        x = self.sd(self.conv5(x))\n",
    "        # x = self.sd(x)\n",
    "        print(f\"Shape after conv5: {x.shape}\")\n",
    "\n",
    "        # x = self.dropout1(self.leaky_relu(self.conv1(x)))\n",
    "        # print(f\"Shape after conv1: {x.shape}\")\n",
    "        # x = self.dropout2(self.leaky_relu(self.conv2(x)))\n",
    "        # print(f\"Shape after conv2: {x.shape}\")\n",
    "        # x = self.bn2(x)\n",
    "        # print(f\"Shape after bn2: {x.shape}\")\n",
    "        # x = x.view(-1, 128 * 7 * 7)\n",
    "        # print(f\"Shape after view: {x.shape}\")\n",
    "        # x = self.sigmoid(self.fc(x))\n",
    "        # print(f\"Shape after fc: {x.shape}\")\n",
    "        return x.view(-1, 1).squeeze(1)\n",
    "# Example usage\n",
    "# z = torch.randn(4, 100, 1, 1)\n",
    "# z = crypten.cryptensor(z)\n",
    "\n",
    "# G = Generator()\n",
    "# G.encrypt()\n",
    "# outputG = G(z)\n",
    "# print(f\"Generated data shape: {outputG.shape}\\n\")\n",
    "\n",
    "# # input_size = (10, 1, 64, 64)\n",
    "# input_size = (10, 1, 28, 28)\n",
    "# x = crypten.cryptensor(torch.randn(input_size))\n",
    "# D = Discriminator(nc=1, ndf=64)\n",
    "# D.encrypt()\n",
    "# outputD = D(x)\n",
    "\n",
    "# print(f\"Discriminator output shape: {outputD.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 缩小网络结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crypten\n",
    "import crypten.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "# class Generator(crypten.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Generator, self).__init__()\n",
    "#         self.decon1 = crypten.nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0,bias=False)\n",
    "#         self.bn1 = crypten.nn.BatchNorm2d(512)\n",
    "#         self.decon2 = crypten.nn.ConvTranspose2d(512, 256, 4, stride=2, padding=1, bias=False)  \n",
    "#         self.bn2 = crypten.nn.BatchNorm2d(256)\n",
    "#         self.decon3 = crypten.nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)  \n",
    "#         self.bn3 = crypten.nn.BatchNorm2d(128)\n",
    "#         self.decon4 = crypten.nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False)  \n",
    "#         self.bn4 = crypten.nn.BatchNorm2d(64)\n",
    "#         self.decon5 = crypten.nn.ConvTranspose2d(64, nc, 4, stride=2, padding=1, bias=False)\n",
    "#         self.relu = crypten.nn.ReLU()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.relu(self.decon1(x))\n",
    "#         x = self.bn1(x)\n",
    "#         # print(f\"Shape after decon1: {x.shape}\")\n",
    "#         x = self.relu(self.decon2(x))\n",
    "#         x = self.bn2(x)\n",
    "#         # print(f\"Shape after deconv2: {x.shape}\")\n",
    "#         x = self.relu(self.decon3(x))\n",
    "#         x = self.bn3(x)\n",
    "#         # print(f\"Shape after deconv3: {x.shape}\")\n",
    "#         x = self.relu(self.decon4(x))\n",
    "#         x = self.bn4(x)\n",
    "#         x = self.decon5(x)\n",
    "#         # print(f\"Shape after deconv5: {x.shape}\")\n",
    "#         x = x.tanh()\n",
    "#         return x\n",
    "\n",
    "# class Discriminator(crypten.nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "\n",
    "#         super(Discriminator, self).__init__()\n",
    "\n",
    "\n",
    "#         # self.nc(1 or 3) * 64 * 64\n",
    "#         self.conv1 = crypten.nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False)\n",
    "#         self.leaky_relu = crypten.nn.LeakyReLU(0.2)\n",
    "#         # self.ndf(64) * 32 * 32\n",
    "#         self.conv2 = crypten.nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False)\n",
    "#         self.bn1 = crypten.nn.BatchNorm2d(ndf * 2)\n",
    "\n",
    "#         # self.ndf*2(128) * 16 * 16\n",
    "#         self.conv3 = crypten.nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False)\n",
    "#         self.bn2 = crypten.nn.BatchNorm2d(ndf * 4)\n",
    "#         # self.ndf*4(256) * 8 * 8\n",
    "#         self.conv4 = crypten.nn.Conv2d(ndf * 4, ndf * 8, 4, stride=2, padding=1, bias=False)\n",
    "#         self.bn3 = crypten.nn.BatchNorm2d(ndf * 8)\n",
    "#         # self.ndf*8(512) * 4 * 4, stride = 1\n",
    "#         # self.conv5 = crypten.nn.Conv2d(self.ndf * 8, 1, 4, stride=1, padding=0, bias=False) # 如果输入是64x64\n",
    "#         self.conv5 = crypten.nn.Conv2d(ndf * 8, 1, 1, stride=1, padding=0, bias=False) # 如果输入是28x28\n",
    "#         self.sd = crypten.nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, x):\n",
    "        \n",
    "#         x = self.leaky_relu(self.conv1(x))\n",
    "#         # print(f\"Shape after conv1: {x.shape}\")\n",
    "#         x = self.leaky_relu(self.bn1(self.conv2(x)))\n",
    "#         # print(f\"Shape after conv2: {x.shape}\")\n",
    "#         x = self.leaky_relu(self.bn2(self.conv3(x)))\n",
    "#         # print(f\"Shape after conv3: {x.shape}\")\n",
    "#         x = self.leaky_relu(self.bn3(self.conv4(x)))\n",
    "#         # print(f\"Shape after conv4: {x.shape}\")\n",
    "#         x = self.sd(self.conv5(x))\n",
    "#         # x = self.sd(x)\n",
    "#         # print(f\"Shape after conv5: {x.shape}\")\n",
    "\n",
    "#         return x.view(-1, 1).squeeze(1)\n",
    "\n",
    "class Generator(crypten.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.decon1 = crypten.nn.ConvTranspose2d(100, 256, 4, stride=1, padding=0,bias=False)\n",
    "        self.bn1 = crypten.nn.BatchNorm2d(256)\n",
    "        self.decon2 = crypten.nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn2 = crypten.nn.BatchNorm2d(128)\n",
    "        self.decon3 = crypten.nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False)  \n",
    "        self.bn3 = crypten.nn.BatchNorm2d(64)\n",
    "        self.decon4 = crypten.nn.ConvTranspose2d(64, nc, 4, stride=2, padding=1, bias=False)\n",
    "        self.relu = crypten.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.decon1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(self.decon2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(self.decon3(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.decon4(x)\n",
    "        x = x.tanh()\n",
    "        return x\n",
    "    \n",
    "\n",
    "class Discriminator(crypten.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.conv1 = crypten.nn.Conv2d(nc, ndf, 4, stride=2, padding=1, bias=False)  # (64, 14, 14)\n",
    "        self.leaky_relu = crypten.nn.LeakyReLU(0.2)\n",
    "        self.conv2 = crypten.nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1, bias=False)  # (128, 7, 7)\n",
    "        self.bn1 = crypten.nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = crypten.nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1, bias=False)  # (256, 3, 3)\n",
    "        self.bn2 = crypten.nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = crypten.nn.Conv2d(ndf * 4, 1, 3, stride=1, padding=0, bias=False)  # (1, 1, 1)\n",
    "        # self.sd = crypten.nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout(0.3) # new\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "\n",
    "        x = self.leaky_relu(self.bn1(self.conv2(x)))\n",
    "\n",
    "        x = self.leaky_relu(self.bn2(self.conv3(x)))\n",
    "\n",
    "        # x = self.sd(self.conv4(x))\n",
    "        x = self.dropout(self.conv4(x))\n",
    "\n",
    "        return x.view(-1, 1).squeeze(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# 加载预训练的Inception模型\n",
    "inception_model = models.inception_v3(weights='DEFAULT', transform_input=False)  # 使用 weights 代替 pretrained\n",
    "inception_model.eval()\n",
    "\n",
    "# 定义图像预处理操作\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 定义Inception Score计算函数\n",
    "def inception_score(img_list, inception_model, num_splits=10):\n",
    "    inception_model.eval()\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img in img_list:\n",
    "            # 如果图像是单通道，扩展为三个通道\n",
    "            if img.size(0) == 1:  # 检查是否为单通道图像\n",
    "                img = img.expand(3, -1, -1)  # 复制单通道数据到三个通道\n",
    "            \n",
    "            img = transform(img)\n",
    "            img = img.unsqueeze(0)  # 增加batch维度\n",
    "            img = img.to('cpu')  # 如果inception模型在cpu上\n",
    "            pred = inception_model(img)[0]\n",
    "            preds.append(pred)\n",
    "\n",
    "    preds = torch.stack(preds)\n",
    "    preds = F.softmax(preds, dim=1)\n",
    "    split_scores = []\n",
    "    for k in range(num_splits):\n",
    "        part = preds[k * (len(preds) // num_splits): (k + 1) * (len(preds) // num_splits), :]\n",
    "        p_y = part.mean(0)\n",
    "        kl_divergence = part * (torch.log(part) - torch.log(p_y.unsqueeze(0)))\n",
    "        kl_divergence = kl_divergence.sum(1)\n",
    "        split_scores.append(kl_divergence.mean().exp())\n",
    "\n",
    "    return torch.tensor(split_scores).mean().item(), torch.tensor(split_scores).std().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(tensor):\n",
    "    mean = tensor.mean()\n",
    "    std = tensor.std()\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def min_max_normalize(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    return (tensor - min_val) / (max_val - min_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([500, 28, 28])\n",
      "torch.Size([500, 28, 28])\n",
      "=====combined_size============combined_size======= torch.Size([1000, 1, 28, 28]) \n",
      "torch.Size([1000, 1, 28, 28])\n",
      "Epoch 0 in progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-4:\n",
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "  File \"/tmp/ipykernel_4671/3334710038.py\", line 133, in run_encrypted_training\n",
      "    d_loss.backward()\n",
      "  File \"/tmp/ipykernel_4671/3334710038.py\", line 133, in run_encrypted_training\n",
      "    d_loss.backward()\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 268, in backward\n",
      "    child.backward(grad_input=grad[idx], top_node=False)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 268, in backward\n",
      "    child.backward(grad_input=grad[idx], top_node=False)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 268, in backward\n",
      "    child.backward(grad_input=grad[idx], top_node=False)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 268, in backward\n",
      "    child.backward(grad_input=grad[idx], top_node=False)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 268, in backward\n",
      "    child.backward(grad_input=grad[idx], top_node=False)\n",
      "  [Previous line repeated 13 more times]\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 268, in backward\n",
      "    child.backward(grad_input=grad[idx], top_node=False)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 255, in backward\n",
      "    grad = self.grad_fn.backward(self.ctx, self.grad)\n",
      "  [Previous line repeated 13 more times]\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/gradients.py\", line 2228, in backward\n",
      "    grad_input = clip_gradient(grad_input, max_norm=1.0)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 255, in backward\n",
      "    grad = self.grad_fn.backward(self.ctx, self.grad)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/gradients.py\", line 2239, in clip_gradient\n",
      "    if grad_norm> max_norm:\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/gradients.py\", line 2228, in backward\n",
      "    grad_input = clip_gradient(grad_input, max_norm=1.0)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 565, in __bool__\n",
      "    raise RuntimeError(\"Cannot evaluate CrypTensors to boolean values\")\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/gradients.py\", line 2239, in clip_gradient\n",
      "    if grad_norm> max_norm:\n",
      "RuntimeError: Cannot evaluate CrypTensors to boolean values\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 565, in __bool__\n",
      "    raise RuntimeError(\"Cannot evaluate CrypTensors to boolean values\")\n",
      "RuntimeError: Cannot evaluate CrypTensors to boolean values\n",
      "ERROR:root:One of the parties failed. Check past logs\n"
     ]
    }
   ],
   "source": [
    "# Example: Stochastic Gradient Descent in CrypTen\n",
    "import pickle as pkl\n",
    "import crypten.mpc as mpc\n",
    "import crypten.communicator as comm\n",
    "\n",
    "\n",
    "@mpc.run_multiprocess(world_size=2)\n",
    "def run_encrypted_training():\n",
    "    # load data\n",
    "    x_alice_enc = crypten.load_from_party('../data/alice_mnist.pth', src=ALICE)\n",
    "    x_bob_enc = crypten.load_from_party('../data/bob_mnist.pth', src=BOB)\n",
    "\n",
    "    crypten.print(x_alice_enc.size())\n",
    "    crypten.print(x_bob_enc.size())\n",
    "\n",
    "    # Combine the feature sets: identical to Tutorial 3\n",
    "    x_combined_enc = crypten.cat([x_alice_enc, x_bob_enc], dim=0)\n",
    "\n",
    "    # Reshape to match the network architecture\n",
    "    x_combined_enc = x_combined_enc.unsqueeze(1)\n",
    "\n",
    "    # # 将通道维度从 1 扩展到 3\n",
    "    # x_combined_enc = torch.cat([x_combined_enc, x_combined_enc, x_combined_enc], dim=1)\n",
    "    \n",
    "    size = x_combined_enc.size()\n",
    "    print(\"=====combined_size=======\", size)\n",
    "\n",
    "\n",
    "    # training hyperparams\n",
    "    learning_rate = 0.005\n",
    "    num_epochs = 1\n",
    "    batch_size = 64\n",
    "    num_batches = x_combined_enc.size(0) // batch_size\n",
    "\n",
    "    print_every = batch_size\n",
    "\n",
    "    # keep track of loss and generated, \"fake\" samples\n",
    "    samples = []\n",
    "    losses = []\n",
    "    img_list = []\n",
    "    img = []\n",
    "    G_losses = []\n",
    "    D_losses = []\n",
    "    loss_tep = 10\n",
    "    SD_FP=[]\n",
    "    SG_FP=[]\n",
    "    SD_BP=[]\n",
    "    SG_BP=[]\n",
    "\n",
    "    # Get some fixed data for sampling. These are images that are held\n",
    "    # constant throughout training, and allow us to inspect the model's performance\n",
    "    sample_size=16\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, nz, 1, 1))\n",
    "    fixed_z = torch.from_numpy(fixed_z).float()\n",
    "    fixed_z = crypten.cryptensor(fixed_z)\n",
    "\n",
    "\n",
    "    G = Generator()\n",
    "    G.encrypt()\n",
    "\n",
    "    D = Discriminator()\n",
    "    D.encrypt()\n",
    "\n",
    "    # define the optimizer\n",
    "\n",
    "    # optimizerG = crypten.optim.Adam(G.parameters(), lr=2e-4)\n",
    "    # optimizerD = crypten.optim.Adam(D.parameters(), lr=2e-4)\n",
    "    optimizerG = crypten.optim.SGD(G.parameters(), lr=2e-4)\n",
    "    optimizerD = crypten.optim.SGD(D.parameters(), lr=2e-4)\n",
    "    \n",
    "    D.train() # Change to training mode\n",
    "    G.train()\n",
    "\n",
    "    rank = comm.get().get_rank()\n",
    "    start_time = time.time()  # 记录training开始的时间\n",
    "    for i in range(num_epochs):\n",
    "        crypten.print(f\"Epoch {i} in progress:\")\n",
    "        \n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # define the start and end of the training mini_batch\n",
    "            start, end = batch * batch_size, (batch + 1) * batch_size\n",
    "\n",
    "            # construct CrypTensor out of training examples / labels\n",
    "            real_images = x_combined_enc[start:end] * 2 - 1 # rescale input\n",
    "            batch_size = real_images.size(0)\n",
    "\n",
    "\n",
    "            # ===========================================\n",
    "            #            TRAIN THR DISCRIMINATOR\n",
    "            # ===========================================\n",
    "            # perform forward pass:\n",
    "            D.zero_grad()\n",
    "\n",
    "            # 1. Train with real images\n",
    "\n",
    "            # Compute the discriminator losses on real images\n",
    "            # smooth the real labels\n",
    "            D_real = D(real_images)\n",
    "            d_real_loss = real_loss(D_real, smooth=True)\n",
    "\n",
    "            # 2. Train with fake images\n",
    "\n",
    "            # Generate fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, nz, 1, 1)) # size大小的矩阵\n",
    "            z = torch.from_numpy(z).float()\n",
    "            z = crypten.cryptensor(z)\n",
    "\n",
    "            g_time = time.time()\n",
    "            fake_images = G(z)\n",
    "            g_end_time = time.time()\n",
    "            # SG_FP.append(min_max_normalize(fake_images).get_plain_text())\n",
    "            # with open('experiment/SG_FP.pkl', 'wb') as f:\n",
    "            #     pkl.dump(SG_FP, f)            \n",
    "            # print(\"Runtime\\nSynthesis:\", g_end_time-g_time)\n",
    "            \n",
    "\n",
    "            # Compute the discriminator losses on fake images\n",
    "            d_time = time.time()\n",
    "            D_fake = D(fake_images)\n",
    "            # SD_FP.append(min_max_normalize(D_fake).get_plain_text())\n",
    "            # with open('experiment/SD_FP.pkl', 'wb') as f:\n",
    "            #     pkl.dump(SD_FP, f)  \n",
    "            # d_end_time = time.time()\n",
    "            # print(\"Runtime\\nInference:\", d_end_time-d_time)\n",
    "\n",
    "            d_fake_loss = fake_loss(D_fake)\n",
    "\n",
    "            # add up loss and perform backprop\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "            rank = comm.get().get_rank()\n",
    "            # crypten.print(f\"\\nRank{rank}:\\n d_loss_dec:{d_loss.get_plain_text()}\", in_order=True)\n",
    "            d_loss.backward()\n",
    "            # original_d_grads = [p for p in D.parameters()]\n",
    "            # for crypten_param in enumerate(original_d_grads):           \n",
    "            #     if isinstance(crypten_param, crypten.CrypTensor):\n",
    "            #         SD_BP.append(crypten_param.get_plain_text())\n",
    "            #     else:\n",
    "            #         SD_BP.append(crypten_param)\n",
    "            # with open('experiment/SD_BP.pkl', 'wb') as f:\n",
    "            #     pkl.dump(SD_BP, f)  \n",
    "\n",
    "            optimizerD.step()\n",
    "            # D.update_parameters(8e-4)\n",
    "            print(\"=========After Discriminator's Backward========\")\n",
    "\n",
    "            # ========================================\n",
    "            #            TRAIN THE GENERATOR\n",
    "            # ========================================\n",
    "            G.zero_grad()\n",
    "            # Train with fake images and flipped labels\n",
    "\n",
    "            # Generate fake images\n",
    "            z = np.random.uniform(-1, 1, size=(batch_size, nz, 1, 1))\n",
    "            z = torch.from_numpy(z).float()\n",
    "            z = crypten.cryptensor(z)\n",
    "            fake_images = G(z)\n",
    "\n",
    "            # Compute the discriminator losses on fake images\n",
    "            D_fake = D(fake_images)\n",
    "            g_loss = real_loss(D_fake)\n",
    "\n",
    "            # perform backprop\n",
    "            g_loss.backward()\n",
    "            # original_g_grads = [p for p in G.parameters()]\n",
    "            # for crypten_param in enumerate(original_g_grads):  \n",
    "            #     if isinstance(crypten_param, crypten.CrypTensor):\n",
    "            #         SG_BP.append(crypten_param.get_plain_text())\n",
    "            #     else:\n",
    "            #         SG_BP.append(crypten_param)\n",
    "            # with open('experiment/SG_BP.pkl', 'wb') as f:\n",
    "            #     pkl.dump(SG_BP, f)  \n",
    "\n",
    "            print(\"=========After Generator's Backward========\")\n",
    "            optimizerG.step()\n",
    "            # G.update_parameters(2e-4)\n",
    "\n",
    "            # print(f'Epoch:[{i+1:0>{num_epochs}}]',\n",
    "            #       f'Step:[{i+1:0>{num_batches}}]',\n",
    "            #       f'Loss-D:{d_loss.get_plain_text().item():.4f}',\n",
    "            #       f'Loss-G:{g_loss.get_plain_text().item():.4f}',\n",
    "            #     #   f'D(x):{D_x:.4f}',\n",
    "            #     #   f'D(G(z)):[{D_G_z1:.4f}/{D_G_z2:.4f}]',\n",
    "            #       f'Time:{run_time}s',\n",
    "            #       end='\\r')\n",
    "\n",
    "            # Print some loss stats\n",
    "            if start % print_every == 0:\n",
    "                # print discriminator and generator loss\n",
    "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                        i+1, num_epochs, d_loss.get_plain_text().item(), g_loss.get_plain_text().item()))\n",
    "\n",
    "            # Save Losses for plotting later\n",
    "            G_losses.append(g_loss.get_plain_text().item())\n",
    "            D_losses.append(d_loss.get_plain_text().item())                \n",
    "            \n",
    "            # 保存最好的模型\n",
    "            if g_loss.get_plain_text() < loss_tep:\n",
    "                torch.save(G.state_dict(), 'LPGAN.pt')\n",
    "                temp = g_loss.get_plain_text()\n",
    "\n",
    "\n",
    "        # generate and save sample, fake images\n",
    "        G.eval()\n",
    "        samples_z = G(fixed_z)\n",
    "        samples.append(samples_z.get_plain_text())\n",
    "        G.train() # back to train model\n",
    "    \n",
    "        # 创建一批潜在向量，我们将使用它们来可视化生成器的生成过程\n",
    "        fixed_noise = crypten.cryptensor(torch.randn(100, nz, 1, 1))\n",
    "\n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        with torch.no_grad():\n",
    "            fake = G(fixed_noise).detach().cpu()\n",
    "            fake = fake.get_plain_text()\n",
    "        img_list.append(utils.make_grid(fake * 0.5 + 0.5, nrow=10))\n",
    "        img.extend(fake)  # 保存每张生成图像\n",
    "        print()\n",
    "\n",
    "    end_time = time.time()\n",
    "    run_time = round(end_time - start_time)\n",
    "    print(\"Trainning time:\", run_time)\n",
    "    # Save training generator samples\n",
    "    with open('train_samples.pkl', 'wb') as f:\n",
    "        pkl.dump(samples, f)\n",
    "\n",
    "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "    plt.plot(G_losses[::100], label=\"G\")\n",
    "    plt.plot(D_losses[::100], label=\"D\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.axhline(y=0, label=\"0\", c=\"g\")  # asymptote\n",
    "    plt.legend()\n",
    "\n",
    "    # 在训练循环结束后评估Inception Score\n",
    "    mean_inception_score, std_inception_score = inception_score(img, inception_model)\n",
    "    print(f\"Inception Score: {mean_inception_score:.4f} ± {std_inception_score:.4f}\")\n",
    "    \n",
    "    \n",
    "run_encrypted_training()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for viewing a list of passed in sample images\n",
    "def view_samples(epoch, samples):\n",
    "    fig, axes = plt.subplots(figsize=(7,7), nrows=4, ncols=4, sharey=True, sharex=True)\n",
    "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
    "        img = img.detach()\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "        im = ax.imshow(img.reshape((32,32)), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "# Load samples from generator, taken while training\n",
    "with open('train_samples.pkl', 'rb') as f:\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 32]' is invalid for input of size 784",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# -1 indicates final epoch's samples (the last in the list)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mview_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamples\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m, in \u001b[0;36mview_samples\u001b[0;34m(epoch, samples)\u001b[0m\n\u001b[1;32m      6\u001b[0m ax\u001b[38;5;241m.\u001b[39mxaxis\u001b[38;5;241m.\u001b[39mset_visible(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m ax\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mset_visible(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 8\u001b[0m im \u001b[38;5;241m=\u001b[39m ax\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGreys_r\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[32, 32]' is invalid for input of size 784"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmUAAAJGCAYAAAAat/K/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCfklEQVR4nO3df3BU5aH/8c8mmg1w3TWQkk1wMVEoSiuJhpKG0bHqDhsv4yV/tBecaYMZxZZeOuLOEI0XkirMRKll0Jo2Fo3A2BZ0ruIfOEHcMTgtMRlBBrHoiIIJhV1+aLIS66aTnO8fDtvvmg1yDot5krxfM2faPfucJ8/T/Uznw9ndxGVZliUAAAAMq4zhXgAAAAAoZQAAAEaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAY4JLhXkA6DAwM6NixY7rsssvkcrmGezlIE8uy9Pnnn6ugoEAZGRf+74d4PK54PJ54PDAwoE8//VSTJk0iN6MIuYFdZAZOpDs3Zycd8bq6uixJHKP06OrqSktO6uvrh30vHOSGw9yDzHAMZ24sy7JcljXyf6N/T0+PLr/8cnV1dcnj8Qz3cpAmsVhMfr9f3d3d8nq9Fzzf1//12tPTo6lTp5KbUYbcwC4yAyfSnRtplLx9efZ2sMfjIfCjULpu97vdbrnd7kHnyc3oRG5gF5mBE+l8S5oP+gMAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGCAS4Z7AcC3JR6PKx6PJx7HYrFhXA1GCnIDu8gMnOJOGcaMhoYGeb3exOH3+4d7SRgByA3sIjNwymVZljXci7hQsVhMXq9XPT098ng8w70cpEm6X9dU/3r1+/3kZpQhN7CLzMCJi9E9ePsSY4bb7Zbb7R7uZWCEITewi8zAKd6+BAAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMMAlw70A4NsSj8cVj8cTj2Ox2DCuBiMFuYFdZAZOcacMY0ZDQ4O8Xm/i8Pv9w70kjADkBnaRGTjlsizLGu5FXKhYLCav16uenh55PJ7hXg7SJN2va6p/vfr9fnIzypAb2EVm4MTF6B68fYkxw+12y+12D/cyMMKQG9hFZuAUb18CAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAY4JLhXgDwbYnH44rH44nHsVhsGFeDkYLcwC4yA6e4U4Yxo6GhQV6vN3H4/f7hXhJGAHIDu8gMnHJZlmUN9yIuVCwWk9frVU9Pjzwez3AvB2mS7tc11b9e/X4/uRllyA3sIjNw4mJ0D96+xJjhdrvldruHexkYYcgN7CIzcIq3LwEAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxwyXAvIB0sy5IkxWKxYV4J0uns63n29b1Q8Xhc8Xg88binpyfp52B0IDewi8zAiXTn5uxkI15XV5cliWOUHl1dXWnJSX19/bDvhePbOz766CNyw0FmOEZMbizLslyWlc6KNzwGBgZ07NgxXXbZZXK5XMO9HKSJZVn6/PPPVVBQoIyMC3+n/ev/eu3u7taVV16pzs5Oeb3eC57fZLFYTH6/X11dXfJ4PMO9nIuqp6dHU6dO1WeffabLL7/8gucbq7khM86N1cxI5OZCjYq3LzMyMnTFFVcM9zJwEaTz/8DcbrfcbnfKnzHa/8/jLI/HM2b2mo4iL5EbMmPfWM+MRG4cz5W2mQAAAOAYpQwAAMAAlDKMWW63W/X19SnfZhht2OvImd8UY2WfEplJJ/Z6YUbFB/0BAABGOu6UAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGMB2KXvzzTd1xx13qKCgQC6XS9u2bfvGa1pbW3XDDTfI7XZr2rRp2rhx46AxjY2NKiwsVHZ2tsrKytTR0WF3aQAAACOW7VLW29ur4uJiNTY2ntf4w4cPa/78+brlllu0b98+LV++XPfcc4927NiRGLN161aFQiHV19dr7969Ki4uVjAY1IkTJ+wuDwAAYERyWZZlOb7Y5dLLL7+sysrKIcc88MAD2r59uw4cOJA4t2jRInV3d6ulpUWSVFZWph/84Ad66qmnJEkDAwPy+/361a9+pQcffNDp8gAAAEaMSy72D2hra1MgEEg6FwwGtXz5cklSX1+f9uzZo9ra2sTzGRkZCgQCamtrSzlnPB5XPB5PPB4YGNCnn36qSZMmyeVypX8TGBaWZenzzz9XQUGBMjIu/OOP5GZsIDewi8zAiXTnRvoWSlkkElFeXl7Suby8PMViMf3zn//UZ599pv7+/pRj3n///ZRzNjQ06OGHH75oa4ZZurq6dMUVV1zwPORmbCE3sIvMwIl05Ub6FkrZxVBbW6tQKJR43NPTo6lTp6qrq0sej2cYV4Z0isVi8vv9uuyyy9IyH7kZG8gN7CIzcCLduZG+hVLm8/kUjUaTzkWjUXk8Ho0bN06ZmZnKzMxMOcbn86Wc0+12y+12Dzrv8XgI/CiUrtv95GZsITewi8zAiXS+JX3Rf09ZeXm5wuFw0rmdO3eqvLxckpSVlaXS0tKkMQMDAwqHw4kxAAAAo53tUnbmzBnt27dP+/btk/TVr7zYt2+fOjs7JX1127aqqiox/he/+IU+/vhj1dTU6P3339fvf/97vfDCC7r//vsTY0KhkDZs2KBNmzbp4MGDWrp0qXp7e1VdXX2B2wMAABgZbL99+fbbb+uWW25JPD77vvnixYu1ceNGHT9+PFHQJKmoqEjbt2/X/fffryeeeEJXXHGFnnnmGQWDwcSYhQsX6uTJk6qrq1MkElFJSYlaWloGffgfAABgtLqg31NmilgsJq/Xq56eHt6vH0Uu9utKbkYncgO7yAycuBivK3/7EgAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADOCpljY2NKiwsVHZ2tsrKytTR0THk2B/96EdyuVyDjvnz5yfG3HXXXYOer6iocLI0AACAEekSuxds3bpVoVBITU1NKisr0/r16xUMBvXBBx9o8uTJg8a/9NJL6uvrSzw+ffq0iouL9ZOf/CRpXEVFhZ577rnEY7fbbXdpAAAAI5btO2Xr1q3TkiVLVF1drZkzZ6qpqUnjx49Xc3NzyvETJ06Uz+dLHDt37tT48eMHlTK32500Licnx9mOAAAARiBbd8r6+vq0Z88e1dbWJs5lZGQoEAiora3tvOZ49tlntWjRIk2YMCHpfGtrqyZPnqycnBzdeuutWrNmjSZNmpRyjng8rng8nngci8XsbANjFLmBE+QGdpEZOGXrTtmpU6fU39+vvLy8pPN5eXmKRCLfeH1HR4cOHDige+65J+l8RUWFNm/erHA4rMcee0y7du3S7bffrv7+/pTzNDQ0yOv1Jg6/329nGxijyA2cIDewi8zAKZdlWdb5Dj527JimTJmi3bt3q7y8PHG+pqZGu3btUnt7+zmv//nPf662tjbt37//nOM+/vhjXX311Xr99dd12223DXo+1b9C/H6/enp65PF4znc7MFwsFpPX603b60puxgZyA7vIDJxId24km29f5ubmKjMzU9FoNOl8NBqVz+c757W9vb3asmWLHnnkkW/8OVdddZVyc3N16NChlKXM7XbzRQDYRm7gBLmBXWQGTtl6+zIrK0ulpaUKh8OJcwMDAwqHw0l3zlJ58cUXFY/H9dOf/vQbf87Ro0d1+vRp5efn21keAADAiGX725ehUEgbNmzQpk2bdPDgQS1dulS9vb2qrq6WJFVVVSV9EeCsZ599VpWVlYM+vH/mzBmtWLFCb731lo4cOaJwOKwFCxZo2rRpCgaDDrcFAAAwstj+PWULFy7UyZMnVVdXp0gkopKSErW0tCQ+/N/Z2amMjOSu98EHH+ivf/2rXnvttUHzZWZmav/+/dq0aZO6u7tVUFCgefPmafXq1dz+BQAAY4btUiZJy5Yt07Jly1I+19raOujcjBkzNNT3CcaNG6cdO3Y4WQYAAMCowd++BAAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADOCpljY2NKiwsVHZ2tsrKytTR0THk2I0bN8rlciUd2dnZSWMsy1JdXZ3y8/M1btw4BQIBffjhh06WBgAAMCLZLmVbt25VKBRSfX299u7dq+LiYgWDQZ04cWLIazwej44fP544Pvnkk6Tn165dqyeffFJNTU1qb2/XhAkTFAwG9eWXX9rfEQAAwAh0id0L1q1bpyVLlqi6ulqS1NTUpO3bt6u5uVkPPvhgymtcLpd8Pl/K5yzL0vr167Vy5UotWLBAkrR582bl5eVp27ZtWrRo0aBr4vG44vF44nEsFrO7DYxB5AZOkBvYRWbglK07ZX19fdqzZ48CgcC/J8jIUCAQUFtb25DXnTlzRldeeaX8fr8WLFig9957L/Hc4cOHFYlEkub0er0qKysbcs6GhgZ5vd7E4ff77WwDYxS5gRPkBnaRGThlq5SdOnVK/f39ysvLSzqfl5enSCSS8poZM2aoublZr7zyip5//nkNDAxo7ty5Onr0qCQlrrMzZ21trXp6ehJHV1eXnW1gjCI3cILcwC4yA6dsv31pV3l5ucrLyxOP586dq2uvvVZPP/20Vq9e7WhOt9stt9udriVijCA3cILcwC4yA6ds3SnLzc1VZmamotFo0vloNDrkZ8a+7tJLL9X111+vQ4cOSVLiuguZEwAAYKSzVcqysrJUWlqqcDicODcwMKBwOJx0N+xc+vv79e677yo/P1+SVFRUJJ/PlzRnLBZTe3v7ec8JAAAw0tl++zIUCmnx4sWaPXu25syZo/Xr16u3tzfxbcyqqipNmTJFDQ0NkqRHHnlEP/zhDzVt2jR1d3frN7/5jT755BPdc889kr76Zuby5cu1Zs0aTZ8+XUVFRVq1apUKCgpUWVmZvp0CAAAYzHYpW7hwoU6ePKm6ujpFIhGVlJSopaUl8UH9zs5OZWT8+wbcZ599piVLligSiSgnJ0elpaXavXu3Zs6cmRhTU1Oj3t5e3Xvvveru7taNN96olpaWQb9kFgAAYLRyWZZlDfciLlQsFpPX61VPT488Hs9wLwdpcrFfV3IzOpEb2EVm4MTFeF3525cAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGMBRKWtsbFRhYaGys7NVVlamjo6OIcdu2LBBN910k3JycpSTk6NAIDBo/F133SWXy5V0VFRUOFkaAADAiGS7lG3dulWhUEj19fXau3eviouLFQwGdeLEiZTjW1tbdeedd+qNN95QW1ub/H6/5s2bp3/84x9J4yoqKnT8+PHE8Ze//MXZjgAAAEYg26Vs3bp1WrJkiaqrqzVz5kw1NTVp/Pjxam5uTjn+T3/6k375y1+qpKRE11xzjZ555hkNDAwoHA4njXO73fL5fIkjJyfH2Y4AAABGoEvsDO7r69OePXtUW1ubOJeRkaFAIKC2trbzmuOLL77Qv/71L02cODHpfGtrqyZPnqycnBzdeuutWrNmjSZNmpRyjng8rng8nngci8XsbANjFLmBE+QGdpEZOGXrTtmpU6fU39+vvLy8pPN5eXmKRCLnNccDDzyggoICBQKBxLmKigpt3rxZ4XBYjz32mHbt2qXbb79d/f39KedoaGiQ1+tNHH6/3842MEaRGzhBbmAXmYFTLsuyrPMdfOzYMU2ZMkW7d+9WeXl54nxNTY127dql9vb2c17/6KOPau3atWptbdWsWbOGHPfxxx/r6quv1uuvv67bbrtt0POp/hXi9/vV09Mjj8dzvtuB4WKxmLxeb9peV3IzNpAb2EVm4ES6cyPZfPsyNzdXmZmZikajSeej0ah8Pt85r3388cf16KOP6vXXXz9nIZOkq666Srm5uTp06FDKUuZ2u+V2u+0sHSA3cITcwC4yA6dsvX2ZlZWl0tLSpA/pn/3Q/v9/5+zr1q5dq9WrV6ulpUWzZ8/+xp9z9OhRnT59Wvn5+XaWBwAAMGLZ/vZlKBTShg0btGnTJh08eFBLly5Vb2+vqqurJUlVVVVJXwR47LHHtGrVKjU3N6uwsFCRSESRSERnzpyRJJ05c0YrVqzQW2+9pSNHjigcDmvBggWaNm2agsFgmrYJAABgNltvX0rSwoULdfLkSdXV1SkSiaikpEQtLS2JD/93dnYqI+PfXe8Pf/iD+vr69OMf/zhpnvr6ev36179WZmam9u/fr02bNqm7u1sFBQWaN2+eVq9eze1fAAAwZtguZZK0bNkyLVu2LOVzra2tSY+PHDlyzrnGjRunHTt2OFkGAADAqMHfvgQAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAI5KWWNjowoLC5Wdna2ysjJ1dHScc/yLL76oa665RtnZ2bruuuv06quvJj1vWZbq6uqUn5+vcePGKRAI6MMPP3SyNAAAgBHJdinbunWrQqGQ6uvrtXfvXhUXFysYDOrEiRMpx+/evVt33nmn7r77br3zzjuqrKxUZWWlDhw4kBizdu1aPfnkk2pqalJ7e7smTJigYDCoL7/80vnOAAAARhDbpWzdunVasmSJqqurNXPmTDU1NWn8+PFqbm5OOf6JJ55QRUWFVqxYoWuvvVarV6/WDTfcoKeeekrSV3fJ1q9fr5UrV2rBggWaNWuWNm/erGPHjmnbtm0XtDkAAICR4hI7g/v6+rRnzx7V1tYmzmVkZCgQCKitrS3lNW1tbQqFQknngsFgonAdPnxYkUhEgUAg8bzX61VZWZna2tq0aNGiQXPG43HF4/HE456eHklSLBazsx0Y7uzraVlWWuYjN2MDuYFdZAZOpDs3ks1SdurUKfX39ysvLy/pfF5ent5///2U10QikZTjI5FI4vmz54Ya83UNDQ16+OGHB533+/3ntxGMKKdPn5bX673gecjN2EJuYBeZgRPpyo1ks5SZora2NunuW3d3t6688kp1dnam7X8YU8ViMfn9fnV1dcnj8Qz3ci6qnp4eTZ06VRMnTkzLfOSG3DgxVnNDZpwbq5mRyM2FslXKcnNzlZmZqWg0mnQ+Go3K5/OlvMbn851z/Nn/jEajys/PTxpTUlKSck632y232z3ovNfrHfUhOMvj8YyZvWZkpOc3t5AbcuPEWM8NmbFvrGdGIjeO57IzOCsrS6WlpQqHw4lzAwMDCofDKi8vT3lNeXl50nhJ2rlzZ2J8UVGRfD5f0phYLKb29vYh5wQAABhtbL99GQqFtHjxYs2ePVtz5szR+vXr1dvbq+rqaklSVVWVpkyZooaGBknSfffdp5tvvlm//e1vNX/+fG3ZskVvv/22/vjHP0qSXC6Xli9frjVr1mj69OkqKirSqlWrVFBQoMrKyvTtFAAAwGC2S9nChQt18uRJ1dXVKRKJqKSkRC0tLYkP6nd2dibdyps7d67+/Oc/a+XKlXrooYc0ffp0bdu2Td///vcTY2pqatTb26t7771X3d3duvHGG9XS0qLs7OzzWpPb7VZ9fX3K28WjDXsdOfObhL2OnPlNMVb2KZGZdGKvF8ZlpfO7nAAAAHCEv30JAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgANul7M0339Qdd9yhgoICuVwubdu27RuvaW1t1Q033CC3261p06Zp48aNg8Y0NjaqsLBQ2dnZKisrU0dHh92lAQAAjFi2S1lvb6+Ki4vV2Nh4XuMPHz6s+fPn65ZbbtG+ffu0fPly3XPPPdqxY0dizNatWxUKhVRfX6+9e/equLhYwWBQJ06csLs8AACAEcllWZbl+GKXSy+//LIqKyuHHPPAAw9o+/btOnDgQOLcokWL1N3drZaWFklSWVmZfvCDH+ipp56SJA0MDMjv9+tXv/qVHnzwwUFzxuNxxePxxOOBgQF9+umnmjRpklwul9PtwDCWZenzzz9XQUGBMjIu/J12cjM2kBvYRWbgRLpzc3ZSxyRZL7/88jnH3HTTTdZ9992XdK65udnyeDyWZVlWPB63MjMzB81TVVVl/dd//VfKOevr6y1JHGPk6OrqchpRcjOGD3LDQWY4RlJuLMuyLvqdsu9+97uqrq5WbW1t4tyrr76q+fPn64svvtBnn32mKVOmaPfu3SovL0+Mqamp0a5du9Te3j5ozq//K6Snp0dTp05VV1eXPB6P0+3AMLFYTH6/X93d3fJ6vRc8H7kZG8gN7CIzcCLduZGkS9Iyy7fM7XbL7XYPOu/xeAj8KJSu2/3kZmwhN7CLzMCJdL4lfdFLmc/nUzQaTToXjUbl8Xg0btw4ZWZmKjMzM+UYn893sZcHAABghIv+e8rKy8sVDoeTzu3cuTPxVmVWVpZKS0uTxgwMDCgcDie9nQkAADCa2S5lZ86c0b59+7Rv3z5JX/3Ki3379qmzs1OSVFtbq6qqqsT4X/ziF/r4449VU1Oj999/X7///e/1wgsv6P7770+MCYVC2rBhgzZt2qSDBw9q6dKl6u3tVXV19QVuDwAAYGSw/fbl22+/rVtuuSXxOBQKSZIWL16sjRs36vjx44mCJklFRUXavn277r//fj3xxBO64oor9MwzzygYDCbGLFy4UCdPnlRdXZ0ikYhKSkrU0tKivLy8C9kbAADAiHFB3740RSwWk9frVU9PDx+iHEUu9utKbkYncgO7yAycuBivK3/7EgAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADOCpljY2NKiwsVHZ2tsrKytTR0THk2B/96EdyuVyDjvnz5yfG3HXXXYOer6iocLI0AACAEekSuxds3bpVoVBITU1NKisr0/r16xUMBvXBBx9o8uTJg8a/9NJL6uvrSzw+ffq0iouL9ZOf/CRpXEVFhZ577rnEY7fbbXdpAAAAI5btO2Xr1q3TkiVLVF1drZkzZ6qpqUnjx49Xc3NzyvETJ06Uz+dLHDt37tT48eMHlTK32500Licnx9mOAAAARiBbd8r6+vq0Z88e1dbWJs5lZGQoEAiora3tvOZ49tlntWjRIk2YMCHpfGtrqyZPnqycnBzdeuutWrNmjSZNmpRyjng8rng8nngci8XsbANjFLmBE+QGdpEZOGXrTtmpU6fU39+vvLy8pPN5eXmKRCLfeH1HR4cOHDige+65J+l8RUWFNm/erHA4rMcee0y7du3S7bffrv7+/pTzNDQ0yOv1Jg6/329nGxijyA2cIDewi8zAKZdlWdb5Dj527JimTJmi3bt3q7y8PHG+pqZGu3btUnt7+zmv//nPf662tjbt37//nOM+/vhjXX311Xr99dd12223DXo+1b9C/H6/enp65PF4znc7MFwsFpPX603b60puxgZyA7vIDJxId24km29f5ubmKjMzU9FoNOl8NBqVz+c757W9vb3asmWLHnnkkW/8OVdddZVyc3N16NChlKXM7XbzRQDYRm7gBLmBXWQGTtl6+zIrK0ulpaUKh8OJcwMDAwqHw0l3zlJ58cUXFY/H9dOf/vQbf87Ro0d1+vRp5efn21keAADAiGX725ehUEgbNmzQpk2bdPDgQS1dulS9vb2qrq6WJFVVVSV9EeCsZ599VpWVlYM+vH/mzBmtWLFCb731lo4cOaJwOKwFCxZo2rRpCgaDDrcFAAAwstj+PWULFy7UyZMnVVdXp0gkopKSErW0tCQ+/N/Z2amMjOSu98EHH+ivf/2rXnvttUHzZWZmav/+/dq0aZO6u7tVUFCgefPmafXq1dz+BQAAY4btUiZJy5Yt07Jly1I+19raOujcjBkzNNT3CcaNG6cdO3Y4WQYAAMCowd++BAAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAjkpZY2OjCgsLlZ2drbKyMnV0dAw5duPGjXK5XElHdnZ20hjLslRXV6f8/HyNGzdOgUBAH374oZOlAQAAjEi2S9nWrVsVCoVUX1+vvXv3qri4WMFgUCdOnBjyGo/Ho+PHjyeOTz75JOn5tWvX6sknn1RTU5Pa29s1YcIEBYNBffnll/Z3BAAAMALZLmXr1q3TkiVLVF1drZkzZ6qpqUnjx49Xc3PzkNe4XC75fL7EkZeXl3jOsiytX79eK1eu1IIFCzRr1ixt3rxZx44d07Zt2xxtCgAAYKSxVcr6+vq0Z88eBQKBf0+QkaFAIKC2trYhrztz5oyuvPJK+f1+LViwQO+9917iucOHDysSiSTN6fV6VVZWNuSc8XhcsVgs6QC+CbmBE+QGdpEZOGWrlJ06dUr9/f1Jd7okKS8vT5FIJOU1M2bMUHNzs1555RU9//zzGhgY0Ny5c3X06FFJSlxnZ86GhgZ5vd7E4ff77WwDYxS5gRPkBnaRGTh10b99WV5erqqqKpWUlOjmm2/WSy+9pO985zt6+umnHc9ZW1urnp6exNHV1ZXGFWO0IjdwgtzALjIDpy6xMzg3N1eZmZmKRqNJ56PRqHw+33nNcemll+r666/XoUOHJClxXTQaVX5+ftKcJSUlKedwu91yu912lg6QGzhCbmAXmYFTtu6UZWVlqbS0VOFwOHFuYGBA4XBY5eXl5zVHf3+/3n333UQBKyoqks/nS5ozFoupvb39vOcEAAAY6WzdKZOkUCikxYsXa/bs2ZozZ47Wr1+v3t5eVVdXS5Kqqqo0ZcoUNTQ0SJIeeeQR/fCHP9S0adPU3d2t3/zmN/rkk090zz33SPrqm5nLly/XmjVrNH36dBUVFWnVqlUqKChQZWVl+nYKAABgMNulbOHChTp58qTq6uoUiURUUlKilpaWxAf1Ozs7lZHx7xtwn332mZYsWaJIJKKcnByVlpZq9+7dmjlzZmJMTU2Nent7de+996q7u1s33nijWlpaBv2SWQAAgNHKZVmWNdyLuFCxWExer1c9PT3yeDzDvRykycV+XcnN6ERuYBeZgRMX43Xlb18CAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEclbLGxkYVFhYqOztbZWVl6ujoGHLshg0bdNNNNyknJ0c5OTkKBAKDxt91111yuVxJR0VFhZOlAQAAjEi2S9nWrVsVCoVUX1+vvXv3qri4WMFgUCdOnEg5vrW1VXfeeafeeOMNtbW1ye/3a968efrHP/6RNK6iokLHjx9PHH/5y1+c7QgAAGAEusTuBevWrdOSJUtUXV0tSWpqatL27dvV3NysBx98cND4P/3pT0mPn3nmGf3f//2fwuGwqqqqEufdbrd8Pt95rSEejysejycex2Ixu9vAGERu4AS5gV1kBk7ZulPW19enPXv2KBAI/HuCjAwFAgG1tbWd1xxffPGF/vWvf2nixIlJ51tbWzV58mTNmDFDS5cu1enTp4eco6GhQV6vN3H4/X4728AYRW7gBLmBXWQGTrksy7LOd/CxY8c0ZcoU7d69W+Xl5YnzNTU12rVrl9rb279xjl/+8pfasWOH3nvvPWVnZ0uStmzZovHjx6uoqEgfffSRHnroIf3Hf/yH2tralJmZOWiOVP8K8fv96unpkcfjOd/twHCxWExerzdtryu5GRvIDewiM3Ai3bmRHLx9eSEeffRRbdmyRa2trYlCJkmLFi1K/PfrrrtOs2bN0tVXX63W1lbddtttg+Zxu91yu93fypoxepAbOEFuYBeZgVO23r7Mzc1VZmamotFo0vloNPqNnwd7/PHH9eijj+q1117TrFmzzjn2qquuUm5urg4dOmRneQAAACOWrVKWlZWl0tJShcPhxLmBgQGFw+GktzO/bu3atVq9erVaWlo0e/bsb/w5R48e1enTp5Wfn29neQAAACOW7V+JEQqFtGHDBm3atEkHDx7U0qVL1dvbm/g2ZlVVlWpraxPjH3vsMa1atUrNzc0qLCxUJBJRJBLRmTNnJElnzpzRihUr9NZbb+nIkSMKh8NasGCBpk2bpmAwmKZtAgAAmM32Z8oWLlyokydPqq6uTpFIRCUlJWppaVFeXp4kqbOzUxkZ/+56f/jDH9TX16cf//jHSfPU19fr17/+tTIzM7V//35t2rRJ3d3dKigo0Lx587R69WrekwcAAGOGow/6L1u2TMuWLUv5XGtra9LjI0eOnHOucePGaceOHU6WAQAAMGrwty8BAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMICjUtbY2KjCwkJlZ2errKxMHR0d5xz/4osv6pprrlF2drauu+46vfrqq0nPW5aluro65efna9y4cQoEAvrwww+dLA0AAGBEsl3Ktm7dqlAopPr6eu3du1fFxcUKBoM6ceJEyvG7d+/WnXfeqbvvvlvvvPOOKisrVVlZqQMHDiTGrF27Vk8++aSamprU3t6uCRMmKBgM6ssvv3S+MwAAgBHEdilbt26dlixZourqas2cOVNNTU0aP368mpubU45/4oknVFFRoRUrVujaa6/V6tWrdcMNN+ipp56S9NVdsvXr12vlypVasGCBZs2apc2bN+vYsWPatm3bBW0OAABgpLjEzuC+vj7t2bNHtbW1iXMZGRkKBAJqa2tLeU1bW5tCoVDSuWAwmChchw8fViQSUSAQSDzv9XpVVlamtrY2LVq0aNCc8Xhc8Xg88binp0eSFIvF7GwHhjv7elqWlZb5yM3YQG5gF5mBE+nOjWSzlJ06dUr9/f3Ky8tLOp+Xl6f3338/5TWRSCTl+Egkknj+7LmhxnxdQ0ODHn744UHn/X7/+W0EI8rp06fl9XoveB5yM7aQG9hFZuBEunIj2SxlpqitrU26+9bd3a0rr7xSnZ2dafsfxlSxWEx+v19dXV3yeDzDvZyLqqenR1OnTtXEiRPTMh+5ITdOjNXckBnnxmpmJHJzoWyVstzcXGVmZioajSadj0aj8vl8Ka/x+XznHH/2P6PRqPLz85PGlJSUpJzT7XbL7XYPOu/1ekd9CM7yeDxjZq8ZGen5zS3khtw4MdZzQ2bsG+uZkciN47nsDM7KylJpaanC4XDi3MDAgMLhsMrLy1NeU15enjReknbu3JkYX1RUJJ/PlzQmFoupvb19yDkBAABGG9tvX4ZCIS1evFizZ8/WnDlztH79evX29qq6ulqSVFVVpSlTpqihoUGSdN999+nmm2/Wb3/7W82fP19btmzR22+/rT/+8Y+SJJfLpeXLl2vNmjWaPn26ioqKtGrVKhUUFKiysjJ9OwUAADCY7VK2cOFCnTx5UnV1dYpEIiopKVFLS0vig/qdnZ1Jt/Lmzp2rP//5z1q5cqUeeughTZ8+Xdu2bdP3v//9xJiamhr19vbq3nvvVXd3t2688Ua1tLQoOzv7vNbkdrtVX1+f8nbxaMNeR878JmGvI2d+U4yVfUpkJp3Y64VxWen8LicAAAAc4W9fAgAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABrBdyt58803dcccdKigokMvl0rZt277xmtbWVt1www1yu92aNm2aNm7cOGhMY2OjCgsLlZ2drbKyMnV0dNhdGgAAwIhlu5T19vaquLhYjY2N5zX+8OHDmj9/vm655Rbt27dPy5cv1z333KMdO3YkxmzdulWhUEj19fXau3eviouLFQwGdeLECbvLAwAAGJFclmVZji92ufTyyy+rsrJyyDEPPPCAtm/frgMHDiTOLVq0SN3d3WppaZEklZWV6Qc/+IGeeuopSdLAwID8fr9+9atf6cEHH3S6PAAAgBHjkov9A9ra2hQIBJLOBYNBLV++XJLU19enPXv2qLa2NvF8RkaGAoGA2traUs4Zj8cVj8cTjwcGBvTpp59q0qRJcrlc6d8EhoVlWfr8889VUFCgjIwL//gjuRkbyA3sIjNwIt25kb6FUhaJRJSXl5d0Li8vT7FYTP/85z/12Wefqb+/P+WY999/P+WcDQ0Nevjhhy/ammGWrq4uXXHFFRc8D7kZW8gN7CIzcCJduZG+hVJ2MdTW1ioUCiUe9/T0aOrUqerq6pLH4xnGlSGdYrGY/H6/LrvssrTMR27GBnIDu8gMnEh3bqRvoZT5fD5Fo9Gkc9FoVB6PR+PGjVNmZqYyMzNTjvH5fCnndLvdcrvdg857PB4CPwql63Y/uRlbyA3sIjNwIp1vSV/031NWXl6ucDicdG7nzp0qLy+XJGVlZam0tDRpzMDAgMLhcGIMAADAaGe7lJ05c0b79u3Tvn37JH31Ky/27dunzs5OSV/dtq2qqkqM/8UvfqGPP/5YNTU1ev/99/X73/9eL7zwgu6///7EmFAopA0bNmjTpk06ePCgli5dqt7eXlVXV1/g9gAAAEYG229fvv3227rlllsSj8++b7548WJt3LhRx48fTxQ0SSoqKtL27dt1//3364knntAVV1yhZ555RsFgMDFm4cKFOnnypOrq6hSJRFRSUqKWlpZBH/4HAAAYrS7o95SZIhaLyev1qqenh/frR5GL/bqSm9GJ3MAuMgMnLsbryt++BAAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADOCpljY2NKiwsVHZ2tsrKytTR0THk2B/96EdyuVyDjvnz5yfG3HXXXYOer6iocLI0AACAEekSuxds3bpVoVBITU1NKisr0/r16xUMBvXBBx9o8uTJg8a/9NJL6uvrSzw+ffq0iouL9ZOf/CRpXEVFhZ577rnEY7fbbXdpAAAAI5btUrZu3TotWbJE1dXVkqSmpiZt375dzc3NevDBBweNnzhxYtLjLVu2aPz48YNKmdvtls/nO681xONxxePxxONYLGZ3GxiDyA2cIDewi8zAKVtvX/b19WnPnj0KBAL/niAjQ4FAQG1tbec1x7PPPqtFixZpwoQJSedbW1s1efJkzZgxQ0uXLtXp06eHnKOhoUFerzdx+P1+O9vAGEVu4AS5gV1kBk65LMuyznfwsWPHNGXKFO3evVvl5eWJ8zU1Ndq1a5fa29vPeX1HR4fKysrU3t6uOXPmJM6fvXtWVFSkjz76SA899JD+4z/+Q21tbcrMzBw0T6p/hfj9fvX09Mjj8ZzvdmC4WCwmr9ebtteV3IwN5AZ2kRk4ke7cSA7evrwQzz77rK677rqkQiZJixYtSvz36667TrNmzdLVV1+t1tZW3XbbbYPmcbvdfOYMtpEbOEFuYBeZgVO23r7Mzc1VZmamotFo0vloNPqNnwfr7e3Vli1bdPfdd3/jz7nqqquUm5urQ4cO2VkeAADAiGWrlGVlZam0tFThcDhxbmBgQOFwOOntzFRefPFFxeNx/fSnP/3Gn3P06FGdPn1a+fn5dpYHAAAwYtn+PWWhUEgbNmzQpk2bdPDgQS1dulS9vb2Jb2NWVVWptrZ20HXPPvusKisrNWnSpKTzZ86c0YoVK/TWW2/pyJEjCofDWrBggaZNm6ZgMOhwWwAAACOL7c+ULVy4UCdPnlRdXZ0ikYhKSkrU0tKivLw8SVJnZ6cyMpK73gcffKC//vWveu211wbNl5mZqf3792vTpk3q7u5WQUGB5s2bp9WrV/OePAAAGDMcfdB/2bJlWrZsWcrnWltbB52bMWOGhvqS57hx47Rjxw4nywAAABg1+NuXAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABjAUSlrbGxUYWGhsrOzVVZWpo6OjiHHbty4US6XK+nIzs5OGmNZlurq6pSfn69x48YpEAjoww8/dLI0AACAEcl2Kdu6datCoZDq6+u1d+9eFRcXKxgM6sSJE0Ne4/F4dPz48cTxySefJD2/du1aPfnkk2pqalJ7e7smTJigYDCoL7/80v6OAAAARiDbpWzdunVasmSJqqurNXPmTDU1NWn8+PFqbm4e8hqXyyWfz5c48vLyEs9ZlqX169dr5cqVWrBggWbNmqXNmzfr2LFj2rZtm6NNAQAAjDS2SllfX5/27NmjQCDw7wkyMhQIBNTW1jbkdWfOnNGVV14pv9+vBQsW6L333ks8d/jwYUUikaQ5vV6vysrKhpwzHo8rFoslHcA3ITdwgtzALjIDp2yVslOnTqm/vz/pTpck5eXlKRKJpLxmxowZam5u1iuvvKLnn39eAwMDmjt3ro4ePSpJievszNnQ0CCv15s4/H6/nW1gjCI3cILcwC4yA6cu+rcvy8vLVVVVpZKSEt1888166aWX9J3vfEdPP/204zlra2vV09OTOLq6utK4YoxW5AZOkBvYRWbg1CV2Bufm5iozM1PRaDTpfDQalc/nO685Lr30Ul1//fU6dOiQJCWui0ajys/PT5qzpKQk5Rxut1tut9vO0gFyA0fIDewiM3DK1p2yrKwslZaWKhwOJ84NDAwoHA6rvLz8vObo7+/Xu+++myhgRUVF8vl8SXPGYjG1t7ef95wAAAAjna07ZZIUCoW0ePFizZ49W3PmzNH69evV29ur6upqSVJVVZWmTJmihoYGSdIjjzyiH/7wh5o2bZq6u7v1m9/8Rp988onuueceSV99M3P58uVas2aNpk+frqKiIq1atUoFBQWqrKxM304BAAAMZruULVy4UCdPnlRdXZ0ikYhKSkrU0tKS+KB+Z2enMjL+fQPus88+05IlSxSJRJSTk6PS0lLt3r1bM2fOTIypqalRb2+v7r33XnV3d+vGG29US0vLoF8yCwAAMFq5LMuyhnsRFyoWi8nr9aqnp0cej2e4l4M0udivK7kZncgN7CIzcOJivK787UsAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMAClDAAAwACUMgAAAANQygAAAAxAKQMAADAApQwAAMAAlDIAAAADUMoAAAAMQCkDAAAwAKUMAADAAJQyAAAAA1DKAAAADOColDU2NqqwsFDZ2dkqKytTR0fHkGM3bNigm266STk5OcrJyVEgEBg0/q677pLL5Uo6KioqnCwNAABgRLJdyrZu3apQKKT6+nrt3btXxcXFCgaDOnHiRMrxra2tuvPOO/XGG2+ora1Nfr9f8+bN0z/+8Y+kcRUVFTp+/Hji+Mtf/uJsRwAAACOQ7VK2bt06LVmyRNXV1Zo5c6aampo0fvx4NTc3pxz/pz/9Sb/85S9VUlKia665Rs8884wGBgYUDoeTxrndbvl8vsSRk5PjbEcAAAAj0CV2Bvf19WnPnj2qra1NnMvIyFAgEFBbW9t5zfHFF1/oX//6lyZOnJh0vrW1VZMnT1ZOTo5uvfVWrVmzRpMmTUo5RzweVzweTzyOxWJ2toExitzACXIDu8gMnLJ1p+zUqVPq7+9XXl5e0vm8vDxFIpHzmuOBBx5QQUGBAoFA4lxFRYU2b96scDisxx57TLt27dLtt9+u/v7+lHM0NDTI6/UmDr/fb2cbGKPIDZwgN7CLzMApl2VZ1vkOPnbsmKZMmaLdu3ervLw8cb6mpka7du1Se3v7Oa9/9NFHtXbtWrW2tmrWrFlDjvv444919dVX6/XXX9dtt9026PlU/wrx+/3q6emRx+M53+3AcLFYTF6vN22vK7kZG8gN7CIzcCLduZFsvn2Zm5urzMxMRaPRpPPRaFQ+n++c1z7++ON69NFH9frrr5+zkEnSVVddpdzcXB06dChlKXO73XK73XaWDpAbOEJuYBeZgVO23r7MyspSaWlp0of0z35o//+/c/Z1a9eu1erVq9XS0qLZs2d/4885evSoTp8+rfz8fDvLAwAAGLFsf/syFAppw4YN2rRpkw4ePKilS5eqt7dX1dXVkqSqqqqkLwI89thjWrVqlZqbm1VYWKhIJKJIJKIzZ85Iks6cOaMVK1borbfe0pEjRxQOh7VgwQJNmzZNwWAwTdsEAAAwm623LyVp4cKFOnnypOrq6hSJRFRSUqKWlpbEh/87OzuVkfHvrveHP/xBfX19+vGPf5w0T319vX79618rMzNT+/fv16ZNm9Td3a2CggLNmzdPq1ev5vYvAAAYM2yXMklatmyZli1blvK51tbWpMdHjhw551zjxo3Tjh07nCwDAABg1OBvXwIAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYABKGQAAgAEoZQAAAAaglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEelrLGxUYWFhcrOzlZZWZk6OjrOOf7FF1/UNddco+zsbF133XV69dVXk563LEt1dXXKz8/XuHHjFAgE9OGHHzpZGgAAwIhku5Rt3bpVoVBI9fX12rt3r4qLixUMBnXixImU43fv3q0777xTd999t9555x1VVlaqsrJSBw4cSIxZu3atnnzySTU1Nam9vV0TJkxQMBjUl19+6XxnAAAAI4jtUrZu3TotWbJE1dXVmjlzppqamjR+/Hg1NzenHP/EE0+ooqJCK1as0LXXXqvVq1frhhtu0FNPPSXpq7tk69ev18qVK7VgwQLNmjVLmzdv1rFjx7Rt27YL2hwAAMBIcYmdwX19fdqzZ49qa2sT5zIyMhQIBNTW1pbymra2NoVCoaRzwWAwUbgOHz6sSCSiQCCQeN7r9aqsrExtbW1atGjRoDnj8bji8XjicU9PjyQpFovZ2Q4Md/b1tCwrLfORm7GB3MAuMgMn0p0byWYpO3XqlPr7+5WXl5d0Pi8vT++//37KayKRSMrxkUgk8fzZc0ON+bqGhgY9/PDDg877/f7z2whGlNOnT8vr9V7wPORmbCE3sIvMwIl05UayWcpMUVtbm3T3rbu7W1deeaU6OzvT9j+MqWKxmPx+v7q6uuTxeIZ7ORdVT0+Ppk6dqokTJ6ZlPnJDbpwYq7khM86N1cxI5OZC2Splubm5yszMVDQaTTofjUbl8/lSXuPz+c45/ux/RqNR5efnJ40pKSlJOafb7Zbb7R503uv1jvoQnOXxeMbMXjMy0vObW8gNuXFirOeGzNg31jMjkRvHc9kZnJWVpdLSUoXD4cS5gYEBhcNhlZeXp7ymvLw8abwk7dy5MzG+qKhIPp8vaUwsFlN7e/uQcwIAAIw2tt++DIVCWrx4sWbPnq05c+Zo/fr16u3tVXV1tSSpqqpKU6ZMUUNDgyTpvvvu080336zf/va3mj9/vrZs2aK3335bf/zjHyVJLpdLy5cv15o1azR9+nQVFRVp1apVKigoUGVlZfp2CgAAYDDbpWzhwoU6efKk6urqFIlEVFJSopaWlsQH9Ts7O5Nu5c2dO1d//vOftXLlSj300EOaPn26tm3bpu9///uJMTU1Nert7dW9996r7u5u3XjjjWppaVF2dvZ5rcntdqu+vj7l7eLRhr2OnPlNwl5HzvymGCv7lMhMOrHXC+Oy0vldTgAAADjC374EAAAwAKUMAADAAJQyAAAAA1DKAAAADEApAwAAMMCIKWWNjY0qLCxUdna2ysrK1NHRcc7xL774oq655hplZ2fruuuu06uvvvotrfTC2dnrxo0b5XK5ko7z/VUiw+3NN9/UHXfcoYKCArlcrsQfqT+X1tZW3XDDDXK73Zo2bZo2btx4zvHkJrWRmhsyk15jITMSuUk3cjM0u7kZxBoBtmzZYmVlZVnNzc3We++9Zy1ZssS6/PLLrWg0mnL83/72NyszM9Nau3at9fe//91auXKldemll1rvvvvut7xy++zu9bnnnrM8Ho91/PjxxBGJRL7lVTvz6quvWv/7v/9rvfTSS5Yk6+WXXz7n+I8//tgaP368FQqFrL///e/W7373OyszM9NqaWlJOZ7cjL7ckJn0GSuZsSxyk07kZmh2c5PKiChlc+bMsf7nf/4n8bi/v98qKCiwGhoaUo7/7//+b2v+/PlJ58rKyqyf//znF3Wd6WB3r88995zl9Xq/pdVdPOcT+JqaGut73/te0rmFCxdawWAw5XhyM7pzQ2YuzFjMjGWRmwtFboZmNzepGP/2ZV9fn/bs2aNAIJA4l5GRoUAgoLa2tpTXtLW1JY2XpGAwOOR4UzjZqySdOXNGV155pfx+vxYsWKD33nvv21jut87O60puyI1EZoZCZs6N3KRGbs4tHa+r8aXs1KlT6u/vT/wZp7Py8vIUiURSXhOJRGyNN4WTvc6YMUPNzc165ZVX9Pzzz2tgYEBz587V0aNHv40lf6uGel1jsZj++c9/Jp0nN+RGIjNDITPnRm5SIzfnZic3Q7H9ty9hlvLycpWXlycez507V9dee62efvpprV69ehhXBpORG9hFZuAEubHH+Dtlubm5yszMVDQaTTofjUbl8/lSXuPz+WyNN4WTvX7dpZdequuvv16HDh26GEscVkO9rh6PR+PGjUs6T27IjURmhkJmzo3cpEZuzs1OboZifCnLyspSaWmpwuFw4tzAwIDC4XBS+/7/lZeXJ42XpJ07dw453hRO9vp1/f39evfdd5Wfn3+xljls7Lyu5IbcSGRmKGTm3MhNauTm3NLyujr5FsK3bcuWLZbb7bY2btxo/f3vf7fuvfde6/LLL098rfZnP/uZ9eCDDybG/+1vf7MuueQS6/HHH7cOHjxo1dfXj6ivG9vZ68MPP2zt2LHD+uijj6w9e/ZYixYtsrKzs6333ntvuLZw3j7//HPrnXfesd555x1LkrVu3TrrnXfesT755BPLsizrwQcftH72s58lxp/9uvGKFSusgwcPWo2Njd/4NXVyM7pyQ2bSZ6xkxrLITTqRm/TlJpURUcosy7J+97vfWVOnTrWysrKsOXPmWG+99VbiuZtvvtlavHhx0vgXXnjB+u53v2tlZWVZ3/ve96zt27d/yyt2zs5ely9fnhibl5dn/ed//qe1d+/eYVi1fW+88YYladBxdn+LFy+2br755kHXlJSUWFlZWdZVV11lPffcc+f8GeTmK6MlN2QmvcZCZiyL3KQbuVlsWVZ6cvN1LsuyLId36gAAAJAmxn+mDAAAYCyglAEAABiAUgYAAGAAShkAAIABKGUAAAAGoJQBAAAYgFIGAABgAEoZAACAAShlAAAABqCUAQAAGIBSBgAAYID/B4yu4uMsAmMWAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 700x700 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -1 indicates final epoch's samples (the last in the list)\n",
    "view_samples(-1, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====fss====\n",
      "====fss====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-6:\n",
      "Process Process-5:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 315, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sue/anaconda3/envs/sl/lib/python3.8/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/mpc/context.py\", line 30, in _launch\n",
      "    return_value = func(*func_args, **func_kwargs)\n",
      "  File \"/tmp/ipykernel_69169/1580691762.py\", line 30, in draw\n",
      "    fake = netG(fixed_z)\n",
      "  File \"/tmp/ipykernel_69169/1580691762.py\", line 30, in draw\n",
      "    fake = netG(fixed_z)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 50, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 50, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 534, in forward_function\n",
      "    return object.__getattribute__(self, name)(*tuple(args), **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 534, in forward_function\n",
      "    return object.__getattribute__(self, name)(*tuple(args), **kwargs)\n",
      "  File \"/tmp/ipykernel_69169/4083422819.py\", line 92, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/tmp/ipykernel_69169/4083422819.py\", line 92, in forward\n",
      "    x = self.bn1(x)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 50, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 50, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 534, in forward_function\n",
      "    return object.__getattribute__(self, name)(*tuple(args), **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 534, in forward_function\n",
      "    return object.__getattribute__(self, name)(*tuple(args), **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 3063, in forward\n",
      "    return input.batchnorm(\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/nn/module.py\", line 3063, in forward\n",
      "    return input.batchnorm(\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 338, in autograd_forward\n",
      "    return self.__getattribute__(name)(*args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 338, in autograd_forward\n",
      "    return self.__getattribute__(name)(*args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 317, in autograd_forward_no_ctx\n",
      "    result = grad_fn.forward(ctx, *args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/cryptensor.py\", line 317, in autograd_forward_no_ctx\n",
      "    result = grad_fn.forward(ctx, *args, **kwargs)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/gradients.py\", line 1819, in forward\n",
      "    stats_dimensions.pop(1)\n",
      "  File \"/home/sue/Project/PR-crypten/CrypTen/crypten/gradients.py\", line 1819, in forward\n",
      "    stats_dimensions.pop(1)\n",
      "IndexError: pop index out of range\n",
      "IndexError: pop index out of range\n",
      "ERROR:root:One of the parties failed. Check past logs\n"
     ]
    }
   ],
   "source": [
    "@mpc.run_multiprocess(world_size=2)\n",
    "def draw():\n",
    "    # Size of the Figure\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    # Plot the real images\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Real Images\")\n",
    "    real = next(iter(dataloader))\n",
    "    real = crypten.cryptensor(real[0][:100])\n",
    "    real = real.get_plain_text()\n",
    "    plt.imshow(\n",
    "        # utils.make_grid(real[0][:100] * 0.5 + 0.5, nrow=10).permute(1, 2, 0))\n",
    "        utils.make_grid(real * 0.5 + 0.5, nrow=10).permute(1, 2, 0))\n",
    "\n",
    "    # Load the Best Generative Model\n",
    "    netG = Generator().encrypt()\n",
    "    netG.load_state_dict(crypten.load('LPGAN.pt', map_location=torch.device('cpu')))\n",
    "    netG.eval()\n",
    "\n",
    "    sample_size=16\n",
    "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, nz, 1, 1))\n",
    "    fixed_z = torch.from_numpy(fixed_z).float()\n",
    "    fixed_z = crypten.cryptensor(fixed_z)\n",
    "\n",
    "\n",
    "    # Generate the Fake Images\n",
    "    with torch.no_grad():\n",
    "        fake = netG(fixed_z)\n",
    "        fake = fake.get_plain_text()\n",
    "\n",
    "    # Plot the fake images\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    fake = utils.make_grid(fake * 0.5 + 0.5, nrow=10)\n",
    "    plt.imshow(fake.permute(1, 2, 0))\n",
    "\n",
    "    # Save the comparation result\n",
    "    plt.savefig('result/result.jpg', bbox_inches='tight')\n",
    "\n",
    "draw()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
